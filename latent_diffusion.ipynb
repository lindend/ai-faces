{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_length = 2048\n",
    "embedding_dim = 4\n",
    "image_size = (224, 224)\n",
    "latent_dim_size = (224 // 4, 224 // 4, embedding_dim)\n",
    "beta = 0.25\n",
    "runeager = False\n",
    "small_dataset = True\n",
    "ds_size = 20048\n",
    "batch_size = 32\n",
    "test_size = 5\n",
    "epochs = 1000\n",
    "num_layers = 2\n",
    "# Number of columns in list_attr_celeba.txt\n",
    "num_label_columns = 40\n",
    "\n",
    "steps = 500\n",
    "variance_schedule_start = 0.0001\n",
    "variance_schedule_end = 0.02\n",
    "variance_schedule = [i * (variance_schedule_end - variance_schedule_start) / steps + variance_schedule_start for i in range(steps)]\n",
    "\n",
    "filters = 32\n",
    "\n",
    "version = 1\n",
    "\n",
    "root = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in colab\n"
     ]
    }
   ],
   "source": [
    "try: #If running in colab \n",
    "  import google.colab\n",
    "  !pip install tensorflow==2.10.0\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  !cp -r drive/MyDrive/datasets .\n",
    "  # !cp drive/MyDrive/ai-faces.zip .\n",
    "  # !unzip ai-faces.zip\n",
    "\n",
    "  # Override parameters when running in colab\n",
    "  root = \"drive/MyDrive/\"\n",
    "  small_dataset = True\n",
    "  runeager = False\n",
    "  filters = 32\n",
    "  batch_size = 256\n",
    "except:\n",
    "  print('Not running in colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_output_path = f\"{root}generated-latent-diffusion-v{version}\"\n",
    "model_path = f\"{root}models/latent_diffusion_faces_v{version}.keras\"\n",
    "    \n",
    "ae_model_path = f\"{root}models/vqgan_faces_v8.keras\"\n",
    "ds_suffix = '_small' if small_dataset else ''\n",
    "ds_path = f\"datasets/vqgan_faces_v8{ds_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.losses\n",
    "import keras.optimizers\n",
    "import keras.layers as layers\n",
    "import keras.losses as losses\n",
    "import keras.callbacks as callbacks\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and decoder\n",
    "Encoder should not include vector quantization, however the decoder should. conv2d_12 is the last layer before the quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(layers.Layer):\n",
    "  def call(self, x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "class GroupNormalization(layers.Layer):\n",
    "  def __init__(self, num_groups = 32, epsilon=1e-7, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_groups = num_groups\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    (_, _, _, C) = input_shape\n",
    "    self.channel_weights = self.add_weight(\"channel_weights\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n",
    "    self.channel_biases = self.add_weight(\"channel_biases\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    (_, W, H, C) = x.shape\n",
    "    B = tf.shape(x)[0]\n",
    "    x = tf.reshape(x, shape=(B, W, H, self.num_groups, C // self.num_groups))\n",
    "    mean, var = tf.nn.moments(x, [1, 2, 4], keepdims=True)\n",
    "    x = (x - mean) / tf.sqrt(var + self.epsilon)\n",
    "    x = tf.reshape(x, shape=(B, W, H, C))\n",
    "    x = x * self.channel_weights + self.channel_biases\n",
    "    return x\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(GroupNormalization, self).get_config()\n",
    "    config.update({\n",
    "      \"num_groups\": self.num_groups,\n",
    "      \"epsilon\": self.epsilon\n",
    "    })\n",
    "    return config\n",
    "\n",
    "class VectorQuantization(layers.Layer):\n",
    "  def __init__(self, embedding_length, embedding_dim, beta=0.25, **kwargs):\n",
    "    super(VectorQuantization, self).__init__(**kwargs)\n",
    "    self.embedding_length = embedding_length\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.beta = beta\n",
    "    self.embedding = self.add_weight(\"embedding\",\n",
    "      shape=(embedding_length, embedding_dim),\n",
    "      initializer=tf.random_uniform_initializer(-1.0, 1.0), \n",
    "      trainable=True)\n",
    "\n",
    "  def call(self, input):\n",
    "    (_, w, h, c) = input.shape\n",
    "    B = tf.shape(input)[0]\n",
    "    flat = tf.reshape(input, shape=(B * w * h, c))\n",
    "    flat = tf.tile(flat, [1, self.embedding_length])\n",
    "    flat = tf.reshape(flat, shape=(B * w * h, self.embedding_length, c))\n",
    "    diff = tf.pow(flat - self.embedding, 2)\n",
    "    diff = tf.reduce_sum(diff, axis=-1)\n",
    "    embedding_indexes = tf.argmin(diff, axis=-1)\n",
    "    embedding_indexes = tf.reshape(embedding_indexes, shape=(B, w, h))\n",
    "    quantized_vectors = tf.gather(self.embedding, embedding_indexes)\n",
    "\n",
    "    embedding_loss = tf.reduce_mean((tf.stop_gradient(input) - quantized_vectors) ** 2)\n",
    "    encoding_loss = tf.reduce_mean((input - tf.stop_gradient(quantized_vectors)) ** 2)\n",
    "    self.add_loss(embedding_loss + self.beta * encoding_loss)\n",
    "\n",
    "    # Straight through estimator\n",
    "    quantized_vectors = input + tf.stop_gradient(quantized_vectors - input)\n",
    "    return quantized_vectors\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(VectorQuantization, self).get_config()\n",
    "    config.update({\n",
    "      \"embedding_length\": self.embedding_length,\n",
    "      \"embedding_dim\": self.embedding_dim,\n",
    "      \"beta\": self.beta\n",
    "    })\n",
    "    return config\n",
    "\n",
    "custom_objects = {\n",
    "  \"Swish\": Swish,\n",
    "  \"GroupNormalization\": GroupNormalization,\n",
    "  \"VectorQuantization\": VectorQuantization\n",
    "}\n",
    "\n",
    "autoencoder = keras.models.load_model(ae_model_path, custom_objects, compile=False)\n",
    "encoder = keras.models.Model(autoencoder.input, autoencoder.get_layer(\"conv2d_12\").output, name=\"encoder\")\n",
    "decoder = keras.models.Model(autoencoder.get_layer(\"vector_quantization\").input, autoencoder.output, name=\"decoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset, write it to file to optimize performance runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ds_path):\n",
    "  fname = os.path.join(\"list_attr_celeba.txt\")\n",
    "\n",
    "  with open(fname) as f:\n",
    "    data = f.read()\n",
    "\n",
    "  lines = data.split(\"\\n\")\n",
    "  header = lines[1].split()\n",
    "  lines = lines[2:-1]\n",
    "  raw_data = np.zeros((len(lines), len(header)))\n",
    "  for i, line in enumerate(lines):\n",
    "    line_data = [int(x) for x in line.split()[1:]]\n",
    "    raw_data[i, :] = line_data[:]\n",
    "\n",
    "  label_dataset = tf.data.Dataset.from_tensor_slices(raw_data).batch(batch_size)\n",
    "  image_dataset = keras.utils.image_dataset_from_directory(\n",
    "    f\"img_align_celeba{ds_suffix}\",\n",
    "    label_mode=None,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    smart_resize=True,\n",
    "    shuffle=False)\n",
    "\n",
    "  image_dataset = (image_dataset\n",
    "    .map(lambda x: x / (255. / 2) - 1., num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .map(lambda x: encoder(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "  )\n",
    "\n",
    "  dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "  dataset = dataset.take(ds_size)\n",
    "  dataset.save(ds_path, compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Diffusion(keras.models.Model):\n",
    "  def __init__(self, model,  num_steps, variance_schedule, **kwargs):\n",
    "    super().__init__(kwargs)\n",
    "    self.num_steps = num_steps\n",
    "    self.loss_tracker = keras.metrics.Mean(\"loss\")\n",
    "    self.model = model\n",
    "    self.loss_fn = keras.losses.MSE\n",
    "    self.variance_schedule = variance_schedule\n",
    "    self.alpha = [1 - b for b in variance_schedule]\n",
    "    self.alpha_accumulated = []\n",
    "    total = 1.\n",
    "    for a in self.alpha:\n",
    "      total *= a\n",
    "      self.alpha_accumulated.append(total)\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [self.loss_tracker]\n",
    "\n",
    "  def train_step(self, input):\n",
    "    real_images, labels = input\n",
    "    _, width, height, channels = real_images.shape\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    labels = labels[:batch_size]\n",
    "    t = tf.random.uniform(shape=(batch_size,), minval=0, maxval=self.num_steps, dtype=tf.int32)\n",
    "    t_input = t / self.num_steps\n",
    "    input_shape = (batch_size, width, height, channels)\n",
    "    noise = tf.random.normal(shape=input_shape)\n",
    "    alpha_t = tf.gather(self.alpha_accumulated, t)\n",
    "    alpha_t = tf.reshape(alpha_t, shape=(batch_size, 1, 1, 1))\n",
    "    noise_variance = tf.sqrt(1 - alpha_t) * noise\n",
    "    img_median = tf.sqrt(alpha_t) * real_images\n",
    "    noisy_input = img_median + noise_variance\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predicted_noise = self.model([noisy_input, labels, t_input])\n",
    "      noise_loss = self.loss_fn(noise, predicted_noise)\n",
    "    grads = tape.gradient(noise_loss, self.model.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "    self.loss_tracker.update_state(noise_loss)\n",
    "\n",
    "    return {\n",
    "      \"loss\": self.loss_tracker.result()\n",
    "    }\n",
    "\n",
    "  def sample(self, num_images, labels=None):\n",
    "    result_shape = (num_images, *latent_dim_size)\n",
    "    result = tf.random.normal(shape=result_shape)\n",
    "    for t in reversed(range(1, self.num_steps)):\n",
    "      if t > 1:\n",
    "        z = tf.random.normal(shape=result_shape)\n",
    "      else:\n",
    "        z = tf.zeros(shape=result_shape)\n",
    "      \n",
    "      alpha = self.alpha[t]\n",
    "      alpha_t = self.alpha_accumulated[t]\n",
    "      t_input = tf.constant(t / self.num_steps, dtype=tf.float32)\n",
    "      t_input = tf.broadcast_to(t_input, shape=(num_images,))\n",
    "      # Todo: make 40 not hardcoded\n",
    "      if labels is None:\n",
    "        labels = tf.random.uniform(shape=(num_images, 40), minval=0, maxval=1, dtype=tf.int32)\n",
    "      # Convert to -1 or 1\n",
    "      labels = labels * 2 - 1\n",
    "      predicted_noise = self.model([result, labels, t_input])\n",
    "      noise_factor = (1 - alpha) / tf.sqrt(1 - alpha_t)\n",
    "      sigma = tf.sqrt(self.variance_schedule[t])\n",
    "      result = (1 / tf.sqrt(alpha)) * (result - noise_factor * predicted_noise) + sigma * z\n",
    "\n",
    "    return result\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters):\n",
    "  def inner(x):\n",
    "    x = layers.Conv2D(filters=filters, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def time_embedding_proj(shape, idx, dense_dim=8):\n",
    "  def inner(x):\n",
    "    b, w, h, c = shape\n",
    "    x = layers.Dense(units=dense_dim, activation=\"relu\", name=f\"time_embedding_{idx}_0\")(x)\n",
    "    x = layers.Dense(units=w * h, name=f\"time_embedding_{idx}_1\")(x)\n",
    "    x = layers.Reshape(target_shape=(w, h, 1), name=f\"time_embedding_reshape_{idx}\")(x)\n",
    "    x = layers.Conv2D(filters=c, kernel_size=1, activation=\"relu\", name=f\"time_embedding_{idx}_2\")(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def downsample():\n",
    "  def inner(x):\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def upsample(filters):\n",
    "  def inner(x):\n",
    "    x = layers.Conv2DTranspose(kernel_size=4, strides=2, filters=filters, padding=\"same\")(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def dropout(rate):\n",
    "  def inner(x):\n",
    "    x = layers.SpatialDropout2D(rate)(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def unet_layer(filters, next_layer):\n",
    "  def inner(x, labels, time_embedding, idx=0):\n",
    "    labels = layers.Dense(units=8, name=f\"labels_proj_{idx}\")(labels)\n",
    "    _, w, h, c = x.shape\n",
    "    mapped_labels = layers.Dense(units=w * h, name=f\"map_labels_{idx}\")(labels)\n",
    "    mapped_labels = layers.Reshape(target_shape=(w, h, 1))(mapped_labels)\n",
    "    mapped_labels = layers.Conv2D(filters=c, kernel_size=1, name=f\"conv_labels_{idx}\")(mapped_labels)\n",
    "    x = layers.add([x, mapped_labels])\n",
    "    x = conv_block(filters)(x)\n",
    "    # In bottom layer, do self attention\n",
    "    if next_layer is None:\n",
    "      x = self_attention(3)(x)\n",
    "    x = dropout(0.2)(x)\n",
    "    x = conv_block(filters)(x)\n",
    "    residual = x\n",
    "    if next_layer is not None:\n",
    "      x = downsample()(x)\n",
    "      x = next_layer(x, labels, filters * 2, time_embedding, idx + 1)\n",
    "      x = upsample(filters)(x)\n",
    "      time = time_embedding_proj(x.shape, idx)(time_embedding)\n",
    "      x = layers.add([residual, x, time])\n",
    "      x = conv_block(filters)(x)\n",
    "      x = conv_block(filters)(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def sublayer(next_layer):\n",
    "  def inner(x, labels, filters, time_embedding, idx):\n",
    "    return unet_layer(filters, next_layer)(x, labels, time_embedding, idx)\n",
    "  return inner\n",
    "\n",
    "def positional_encoding2d():\n",
    "  def inner(inputs):\n",
    "    _, w, h, c = inputs.shape\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    x = tf.range(start=0, limit=w, delta=1)\n",
    "    x = x / w\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    assert x.shape == (1, w)\n",
    "    x = tf.tile(x, multiples=[h, 1])\n",
    "    assert x.shape == (w, h)\n",
    "    x = tf.reshape(x, shape=(w, h, 1))\n",
    "\n",
    "    y = tf.range(start=0, limit=h, delta=1)\n",
    "    y = y / h\n",
    "    y = tf.expand_dims(y, axis=1)\n",
    "    assert y.shape == (h, 1)\n",
    "    y = tf.tile(y, [1, w])\n",
    "    assert y.shape == (w, h)\n",
    "    y = tf.reshape(y, shape=(w, h, 1))\n",
    "\n",
    "    indexes = tf.concat([x, y], axis=-1)\n",
    "    assert indexes.shape == (w, h, 2)\n",
    "\n",
    "    indexes = tf.expand_dims(indexes, axis=0)\n",
    "    indexes = tf.tile(indexes, [batch_size, 1, 1, 1])\n",
    "\n",
    "    return layers.Conv2D(c, kernel_size=1, strides=1, padding=\"same\")(indexes)\n",
    "    # Todo: the sinusoidal way from All you need is attention\n",
    "  return inner\n",
    "\n",
    "def self_attention(num_heads=1, key_dim=64):\n",
    "  def inner(x):\n",
    "    # add positional encoding?\n",
    "    pos = positional_encoding2d()(x)\n",
    "    x = layers.add([x, pos])\n",
    "    x = layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(x, x, x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def cross_attention(num_heads=1, key_dim=64):\n",
    "  def inner(qk, v):\n",
    "    pos_qk = positional_encoding2d()(qk)\n",
    "    pos_v = positional_encoding2d()(v)\n",
    "    qk = layers.add([qk, pos_qk])\n",
    "    v = layers.add([v, pos_v])\n",
    "    return layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(qk, v, qk)\n",
    "  return inner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(num_labels, filters):\n",
    "  if os.path.exists(model_path):\n",
    "    return keras.models.load_model(model_path)\n",
    "\n",
    "  w, h, c = latent_dim_size\n",
    "  image_input = keras.Input(shape=(w, h, c), name=\"images\")\n",
    "  t_input = keras.Input(shape=(1,), name=\"t_input\")\n",
    "  label_input = keras.Input(shape=(num_labels,), name=\"labels\")\n",
    "  sublayers = sublayer(None)\n",
    "  for i in range(num_layers):\n",
    "    sublayers = sublayer(sublayers)\n",
    "\n",
    "  x = unet_layer(filters, sublayers)(image_input, label_input, t_input)\n",
    "  output = layers.Conv2D(filters=embedding_dim, kernel_size=3, padding=\"same\")(x)\n",
    "  model = keras.Model([image_input, label_input, t_input], output)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionMonitor(keras.callbacks.Callback):\n",
    "  def __init__(self, decoder, num_img=3):\n",
    "    self.num_img = num_img\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if epoch % 10 != 0:\n",
    "      return\n",
    "    generated_images = self.model.sample(self.num_img)\n",
    "    generated_images = self.decoder(generated_images)\n",
    "    if not os.path.exists(img_output_path):\n",
    "      os.mkdir(img_output_path)\n",
    "\n",
    "    for i in range(self.num_img):\n",
    "      img = keras.utils.array_to_img(generated_images[i])\n",
    "      img.save(os.path.join(img_output_path, f\"generated_{epoch:03d}_{i}.png\"))\n",
    "\n",
    "class Save(keras.callbacks.Callback):\n",
    "  def __init__(self, model_path):\n",
    "    self.model_path = model_path\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.model.model.save(self.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " labels (InputLayer)            [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " labels_proj_0 (Dense)          (None, 8)            328         ['labels[0][0]']                 \n",
      "                                                                                                  \n",
      " map_labels_0 (Dense)           (None, 3136)         28224       ['labels_proj_0[0][0]']          \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 56, 56, 1)    0           ['map_labels_0[0][0]']           \n",
      "                                                                                                  \n",
      " images (InputLayer)            [(None, 56, 56, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " conv_labels_0 (Conv2D)         (None, 56, 56, 4)    8           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 56, 56, 4)    0           ['images[0][0]',                 \n",
      "                                                                  'conv_labels_0[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 56, 56, 32)   1184        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " labels_proj_1 (Dense)          (None, 8)            72          ['labels_proj_0[0][0]']          \n",
      "                                                                                                  \n",
      " spatial_dropout2d (SpatialDrop  (None, 56, 56, 32)  0           ['conv2d[0][0]']                 \n",
      " out2D)                                                                                           \n",
      "                                                                                                  \n",
      " map_labels_1 (Dense)           (None, 784)          7056        ['labels_proj_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 56, 56, 32)   9248        ['spatial_dropout2d[0][0]']      \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 28, 28, 1)    0           ['map_labels_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 28, 28, 32)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv_labels_1 (Conv2D)         (None, 28, 28, 32)   64          ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 32)   0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv_labels_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 28, 64)   18496       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " labels_proj_2 (Dense)          (None, 8)            72          ['labels_proj_1[0][0]']          \n",
      "                                                                                                  \n",
      " spatial_dropout2d_1 (SpatialDr  (None, 28, 28, 64)  0           ['conv2d_2[0][0]']               \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " map_labels_2 (Dense)           (None, 196)          1764        ['labels_proj_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 28, 28, 64)   36928       ['spatial_dropout2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 14, 14, 1)    0           ['map_labels_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv_labels_2 (Conv2D)         (None, 14, 14, 64)   128         ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 14, 14, 64)   0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv_labels_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 14, 14, 128)  73856       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " labels_proj_3 (Dense)          (None, 8)            72          ['labels_proj_2[0][0]']          \n",
      "                                                                                                  \n",
      " spatial_dropout2d_2 (SpatialDr  (None, 14, 14, 128)  0          ['conv2d_4[0][0]']               \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " map_labels_3 (Dense)           (None, 49)           441         ['labels_proj_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 14, 14, 128)  147584      ['spatial_dropout2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 7, 7, 1)      0           ['map_labels_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 128)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv_labels_3 (Conv2D)         (None, 7, 7, 128)    256         ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 7, 7, 128)    0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv_labels_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 7, 7, 256)    295168      ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_2 (TFOpLamb  (4,)                0           ['conv2d_6[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  ()                  0           ['tf.compat.v1.shape_2[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.tile_2 (TFOpLambda)         (None, 7, 7, 2)      0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 7, 7, 256)    768         ['tf.tile_2[0][0]']              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 7, 7, 256)    0           ['conv2d_6[0][0]',               \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " t_input (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 7, 7, 256)   197440      ['add_4[0][0]',                  \n",
      " dAttention)                                                      'add_4[0][0]',                  \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " time_embedding_2_0 (Dense)     (None, 8)            16          ['t_input[0][0]']                \n",
      "                                                                                                  \n",
      " spatial_dropout2d_3 (SpatialDr  (None, 7, 7, 256)   0           ['multi_head_attention[0][0]']   \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " time_embedding_2_1 (Dense)     (None, 196)          1764        ['time_embedding_2_0[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 7, 7, 256)    590080      ['spatial_dropout2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " time_embedding_reshape_2 (Resh  (None, 14, 14, 1)   0           ['time_embedding_2_1[0][0]']     \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 14, 14, 128)  524416     ['conv2d_8[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " time_embedding_2_2 (Conv2D)    (None, 14, 14, 128)  256         ['time_embedding_reshape_2[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 14, 14, 128)  0           ['conv2d_5[0][0]',               \n",
      "                                                                  'conv2d_transpose[0][0]',       \n",
      "                                                                  'time_embedding_2_2[0][0]']     \n",
      "                                                                                                  \n",
      " time_embedding_1_0 (Dense)     (None, 8)            16          ['t_input[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 14, 14, 128)  147584      ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " time_embedding_1_1 (Dense)     (None, 784)          7056        ['time_embedding_1_0[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 14, 128)  147584      ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " time_embedding_reshape_1 (Resh  (None, 28, 28, 1)   0           ['time_embedding_1_1[0][0]']     \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 28, 28, 64)  131136      ['conv2d_10[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " time_embedding_1_2 (Conv2D)    (None, 28, 28, 64)   128         ['time_embedding_reshape_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 28, 28, 64)   0           ['conv2d_3[0][0]',               \n",
      "                                                                  'conv2d_transpose_1[0][0]',     \n",
      "                                                                  'time_embedding_1_2[0][0]']     \n",
      "                                                                                                  \n",
      " time_embedding_0_0 (Dense)     (None, 8)            16          ['t_input[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 28, 28, 64)   36928       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " time_embedding_0_1 (Dense)     (None, 3136)         28224       ['time_embedding_0_0[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 28, 28, 64)   36928       ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " time_embedding_reshape_0 (Resh  (None, 56, 56, 1)   0           ['time_embedding_0_1[0][0]']     \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 56, 56, 32)  32800       ['conv2d_12[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " time_embedding_0_2 (Conv2D)    (None, 56, 56, 32)   64          ['time_embedding_reshape_0[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 56, 56, 32)   0           ['conv2d_1[0][0]',               \n",
      "                                                                  'conv2d_transpose_2[0][0]',     \n",
      "                                                                  'time_embedding_0_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 56, 56, 32)   9248        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 56, 56, 32)   9248        ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 56, 56, 4)    1156        ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,523,805\n",
      "Trainable params: 2,523,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_model(num_label_columns, filters)\n",
    "model.summary()\n",
    "\n",
    "diffusion = Diffusion(model, steps, variance_schedule)\n",
    "diffusion.compile(optimizer=\"rmsprop\", run_eagerly=runeager)\n",
    "\n",
    "callbacks = [\n",
    "  DiffusionMonitor(decoder),\n",
    "  Save(model_path)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.load(ds_path, compression='GZIP')\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.6253WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 35s 660ms/step - loss: 0.6253\n",
      "Epoch 2/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.3000WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.3008\n",
      "Epoch 3/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.3064WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.3064\n",
      "Epoch 4/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.3011WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 57ms/step - loss: 0.2996\n",
      "Epoch 5/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2897WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 57ms/step - loss: 0.2895\n",
      "Epoch 6/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2656WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.2648\n",
      "Epoch 7/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2749WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.2749\n",
      "Epoch 8/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2593WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.2595\n",
      "Epoch 9/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2336WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.2328\n",
      "Epoch 10/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2448WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 56ms/step - loss: 0.2444\n",
      "Epoch 11/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.2228WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 24s 589ms/step - loss: 0.2230\n",
      "Epoch 12/1000\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2253WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 2s 59ms/step - loss: 0.2253\n",
      "Epoch 13/1000\n",
      "15/41 [=========>....................] - ETA: 1s - loss: 0.2237"
     ]
    }
   ],
   "source": [
    "\n",
    "diffusion.fit(dataset, callbacks=callbacks, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32f2fe102a4f10662d8c13f75131e1ba377b7194060421a642fdea27c55fc65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
