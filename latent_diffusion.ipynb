{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_length = 2048\n",
    "embedding_dim = 4\n",
    "image_size = (224, 224)\n",
    "latent_dim_size = (224 // 4, 224 // 4, embedding_dim)\n",
    "beta = 0.25\n",
    "runeager = False\n",
    "small_dataset = True\n",
    "ds_size = 20048\n",
    "batch_size = 4\n",
    "test_size = 5\n",
    "epochs = 1000\n",
    "num_layers = 1\n",
    "# Number of columns in list_attr_celeba.txt\n",
    "num_label_columns = 40\n",
    "\n",
    "steps = 500\n",
    "variance_schedule_start = 0.0001\n",
    "variance_schedule_end = 0.02\n",
    "variance_schedule = [i * (variance_schedule_end - variance_schedule_start) / steps + variance_schedule_start for i in range(steps)]\n",
    "\n",
    "filters = 64\n",
    "\n",
    "version = 3\n",
    "\n",
    "root = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in colab\n"
     ]
    }
   ],
   "source": [
    "try: #If running in colab \n",
    "  import google.colab\n",
    "  !pip install tensorflow==2.10.0\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  !cp -r drive/MyDrive/datasets .\n",
    "  # !cp drive/MyDrive/ai-faces.zip .\n",
    "  # !unzip ai-faces.zip\n",
    "\n",
    "  # Override parameters when running in colab\n",
    "  root = \"drive/MyDrive/\"\n",
    "  small_dataset = False\n",
    "  runeager = False\n",
    "  batch_size = 64\n",
    "  version = 3\n",
    "except:\n",
    "  print('Not running in colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_output_path = f\"{root}generated-latent-diffusion-v{version}\"\n",
    "model_path = f\"{root}models/latent_diffusion_faces_v{version}.keras\"\n",
    "    \n",
    "ae_model_path = f\"{root}models/vqgan_faces_v8.keras\"\n",
    "ds_suffix = '_small' if small_dataset else ''\n",
    "ds_path = f\"datasets/vqgan_faces_v8{ds_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.losses\n",
    "import keras.optimizers\n",
    "import keras.layers as layers\n",
    "import keras.losses as losses\n",
    "import keras.callbacks as callbacks\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "class Swish(layers.Layer):\n",
    "  def call(self, x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "class GroupNormalization(layers.Layer):\n",
    "  def __init__(self, num_groups = 32, epsilon=1e-7, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_groups = num_groups\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    (_, _, _, C) = input_shape\n",
    "    self.channel_weights = self.add_weight(\"channel_weights\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n",
    "    self.channel_biases = self.add_weight(\"channel_biases\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    (_, W, H, C) = x.shape\n",
    "    B = tf.shape(x)[0]\n",
    "    x = tf.reshape(x, shape=(B, W, H, self.num_groups, C // self.num_groups))\n",
    "    mean, var = tf.nn.moments(x, [1, 2, 4], keepdims=True)\n",
    "    x = (x - mean) / tf.sqrt(var + self.epsilon)\n",
    "    x = tf.reshape(x, shape=(B, W, H, C))\n",
    "    x = x * self.channel_weights + self.channel_biases\n",
    "    return x\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(GroupNormalization, self).get_config()\n",
    "    config.update({\n",
    "      \"num_groups\": self.num_groups,\n",
    "      \"epsilon\": self.epsilon\n",
    "    })\n",
    "    return config\n",
    "\n",
    "class VectorQuantization(layers.Layer):\n",
    "  def __init__(self, embedding_length, embedding_dim, beta=0.25, **kwargs):\n",
    "    super(VectorQuantization, self).__init__(**kwargs)\n",
    "    self.embedding_length = embedding_length\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.beta = beta\n",
    "    self.embedding = self.add_weight(\"embedding\",\n",
    "      shape=(embedding_length, embedding_dim),\n",
    "      initializer=tf.random_uniform_initializer(-1.0, 1.0), \n",
    "      trainable=True)\n",
    "\n",
    "  def call(self, input):\n",
    "    (_, w, h, c) = input.shape\n",
    "    B = tf.shape(input)[0]\n",
    "    flat = tf.reshape(input, shape=(B * w * h, c))\n",
    "    flat = tf.tile(flat, [1, self.embedding_length])\n",
    "    flat = tf.reshape(flat, shape=(B * w * h, self.embedding_length, c))\n",
    "    diff = tf.pow(flat - self.embedding, 2)\n",
    "    diff = tf.reduce_sum(diff, axis=-1)\n",
    "    embedding_indexes = tf.argmin(diff, axis=-1)\n",
    "    embedding_indexes = tf.reshape(embedding_indexes, shape=(B, w, h))\n",
    "    quantized_vectors = tf.gather(self.embedding, embedding_indexes)\n",
    "\n",
    "    embedding_loss = tf.reduce_mean((tf.stop_gradient(input) - quantized_vectors) ** 2)\n",
    "    encoding_loss = tf.reduce_mean((input - tf.stop_gradient(quantized_vectors)) ** 2)\n",
    "    self.add_loss(embedding_loss + self.beta * encoding_loss)\n",
    "\n",
    "    # Straight through estimator\n",
    "    quantized_vectors = input + tf.stop_gradient(quantized_vectors - input)\n",
    "    return quantized_vectors\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(VectorQuantization, self).get_config()\n",
    "    config.update({\n",
    "      \"embedding_length\": self.embedding_length,\n",
    "      \"embedding_dim\": self.embedding_dim,\n",
    "      \"beta\": self.beta\n",
    "    })\n",
    "    return config\n",
    "\n",
    "custom_objects = {\n",
    "  \"Swish\": Swish,\n",
    "  \"GroupNormalization\": GroupNormalization,\n",
    "  \"VectorQuantization\": VectorQuantization\n",
    "}\n",
    "\n",
    "autoencoder = keras.models.load_model(ae_model_path, custom_objects, compile=False)\n",
    "encoder = keras.models.Model(autoencoder.input, autoencoder.get_layer(\"conv2d_12\").output, name=\"encoder\")\n",
    "decoder = keras.models.Model(autoencoder.get_layer(\"vector_quantization\").input, autoencoder.output, name=\"decoder\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Prepare dataset, write it to file to optimize performance runtime\n",
    "\n",
    "# %%\n",
    "if not os.path.exists(ds_path):\n",
    "  fname = os.path.join(\"list_attr_celeba.txt\")\n",
    "\n",
    "  with open(fname) as f:\n",
    "    data = f.read()\n",
    "\n",
    "  lines = data.split(\"\\n\")\n",
    "  header = lines[1].split()\n",
    "  lines = lines[2:-1]\n",
    "  raw_data = np.zeros((len(lines), len(header)))\n",
    "  for i, line in enumerate(lines):\n",
    "    line_data = [int(x) for x in line.split()[1:]]\n",
    "    raw_data[i, :] = line_data[:]\n",
    "\n",
    "  label_dataset = tf.data.Dataset.from_tensor_slices(raw_data)\n",
    "  image_dataset = keras.utils.image_dataset_from_directory(\n",
    "    f\"img_align_celeba{ds_suffix}\",\n",
    "    label_mode=None,\n",
    "    image_size=image_size,\n",
    "    smart_resize=True,\n",
    "    batch_size=16,\n",
    "    shuffle=False)\n",
    "\n",
    "  image_dataset = (image_dataset\n",
    "    .map(lambda x: x / (255. / 2) - 1., num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .map(lambda x: encoder(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    .unbatch()\n",
    "  )\n",
    "\n",
    "  dataset = tf.data.Dataset.zip((image_dataset, label_dataset)).batch(batch_size)\n",
    "  dataset = dataset.take(ds_size)\n",
    "  dataset.save(ds_path, compression='GZIP')\n",
    "\n",
    "# %%\n",
    "\n",
    "class Diffusion(keras.models.Model):\n",
    "  def __init__(self, model,  num_steps, variance_schedule, **kwargs):\n",
    "    super().__init__(kwargs)\n",
    "    self.num_steps = num_steps\n",
    "    self.loss_tracker = keras.metrics.Mean(\"loss\")\n",
    "    self.model = model\n",
    "    self.loss_fn = keras.losses.MSE\n",
    "    self.variance_schedule = variance_schedule\n",
    "    self.alpha = [1 - b for b in variance_schedule]\n",
    "    self.alpha_accumulated = []\n",
    "    total = 1.\n",
    "    for a in self.alpha:\n",
    "      total *= a\n",
    "      self.alpha_accumulated.append(total)\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [self.loss_tracker]\n",
    "\n",
    "  def train_step(self, input):\n",
    "    real_images, labels = input\n",
    "    _, width, height, channels = real_images.shape\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    labels = labels[:batch_size]\n",
    "    t = tf.random.uniform(shape=(batch_size,), minval=0, maxval=self.num_steps, dtype=tf.int32)\n",
    "    t_input = t / self.num_steps\n",
    "    input_shape = (batch_size, width, height, channels)\n",
    "    noise = tf.random.normal(shape=input_shape)\n",
    "    alpha_t = tf.gather(self.alpha_accumulated, t)\n",
    "    alpha_t = tf.reshape(alpha_t, shape=(batch_size, 1, 1, 1))\n",
    "    noise_variance = tf.sqrt(1 - alpha_t) * noise\n",
    "    img_median = tf.sqrt(alpha_t) * real_images\n",
    "    noisy_input = img_median + noise_variance\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predicted_noise = self.model([noisy_input, labels, t_input])\n",
    "      noise_loss = self.loss_fn(noise, predicted_noise)\n",
    "    grads = tape.gradient(noise_loss, self.model.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "    self.loss_tracker.update_state(noise_loss)\n",
    "\n",
    "    return {\n",
    "      \"loss\": self.loss_tracker.result()\n",
    "    }\n",
    "\n",
    "  def sample(self, num_images, labels=None):\n",
    "    result_shape = (num_images, *latent_dim_size)\n",
    "    result = tf.random.normal(shape=result_shape)\n",
    "    for t in reversed(range(1, self.num_steps)):\n",
    "      if t > 1:\n",
    "        z = tf.random.normal(shape=result_shape)\n",
    "      else:\n",
    "        z = tf.zeros(shape=result_shape)\n",
    "      \n",
    "      alpha = self.alpha[t]\n",
    "      alpha_t = self.alpha_accumulated[t]\n",
    "      t_input = tf.constant(t / self.num_steps, dtype=tf.float32)\n",
    "      t_input = tf.broadcast_to(t_input, shape=(num_images,))\n",
    "      # Todo: make 40 not hardcoded\n",
    "      if labels is None:\n",
    "        labels = tf.random.uniform(shape=(num_images, 40), minval=0, maxval=1, dtype=tf.int32)\n",
    "      # Convert to -1 or 1\n",
    "      labels = labels * 2 - 1\n",
    "      predicted_noise = self.model([result, labels, t_input])\n",
    "      noise_factor = (1 - alpha) / tf.sqrt(1 - alpha_t)\n",
    "      sigma = tf.sqrt(self.variance_schedule[t])\n",
    "      result = (1 / tf.sqrt(alpha)) * (result - noise_factor * predicted_noise) + sigma * z\n",
    "\n",
    "    return result\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# %%\n",
    "def conv_block(filters, x):\n",
    "  x = GroupNormalization()(x)\n",
    "  x = Swish()(x)\n",
    "  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "\n",
    "  x = GroupNormalization()(x)\n",
    "  x = Swish()(x)\n",
    "  x = layers.Dropout(0.2)(x)\n",
    "  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "  return x\n",
    "\n",
    "def residual_block(filters, x):\n",
    "  residual = x\n",
    "\n",
    "  x = GroupNormalization()(x)\n",
    "  x = Swish()(x)\n",
    "  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "\n",
    "  x = GroupNormalization()(x)\n",
    "  x = Swish()(x)\n",
    "  x = layers.Dropout(0.2)(x)\n",
    "  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "\n",
    "  return layers.Add()([residual, x])\n",
    "\n",
    "def downsample_block(filters, x):\n",
    "  return layers.Conv2D(filters, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "def upsample_block(filters, x):\n",
    "  (_, w, h, _) = x.shape\n",
    "  x = layers.Resizing(h * 2, w * 2)(x)\n",
    "  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "  return x\n",
    "\n",
    "def time_embedding_proj(shape, idx, dense_dim=8):\n",
    "  def inner(x):\n",
    "    b, w, h, c = shape\n",
    "    x = layers.Dense(units=dense_dim, activation=\"relu\", name=f\"time_embedding_{idx}_0\")(x)\n",
    "    x = layers.Dense(units=w * h, name=f\"time_embedding_{idx}_1\")(x)\n",
    "\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def add_embedding(x, emb, emb_dim=16):\n",
    "  (_, w, h, c) = x.shape\n",
    "  b = tf.shape(x)[0]\n",
    "  emb = layers.Dense(units=emb_dim)(emb)\n",
    "  emb = tf.tile(emb, (b, w, h))\n",
    "  x = tf.concat([x, emb])\n",
    "  # kernel_size larger than 1?\n",
    "  x = layers.Conv2D(filters=c, kernel_size=1, padding=\"same\")(x)\n",
    "  x = GroupNormalization()(x)\n",
    "  x = Swish()(x)\n",
    "  return x\n",
    "\n",
    "def add_conditioning(cond, x, num_heads=5, attn_dim=32):\n",
    "  (_, w, h, c) = x.shape\n",
    "  b = tf.shape(x)[0]\n",
    "  x = layers.Reshape(target_shape=(w * h, c))(x)\n",
    "  cond = tf.expand_dims(cond, axis=1)\n",
    "  cond = tf.repeat(cond, w * h, axis=1)\n",
    "  x = layers.MultiHeadAttention(num_heads, attn_dim)(x, cond, cond)\n",
    "  x = layers.Reshape(target_shape=(w, h, c))(x)\n",
    "  return x\n",
    "\n",
    "def unet_layer(filters, next_layer):\n",
    "  def inner(x, labels, time):\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = add_conditioning(labels, x)\n",
    "    x = add_conditioning(time, x)\n",
    "    x = residual_block(filters, x)\n",
    "    # In bottom layer, do self attention\n",
    "    if next_layer is None:\n",
    "      x = self_attention(3)(x)\n",
    "\n",
    "    x = residual_block(filters, x)\n",
    "    residual = x\n",
    "    if next_layer is not None:\n",
    "      x = downsample_block(filters, x)\n",
    "\n",
    "      x = next_layer(x, labels, filters * 2, time)\n",
    "\n",
    "      x = upsample_block(filters, x)\n",
    "\n",
    "      x = layers.Add()([residual, x])\n",
    "\n",
    "      x = residual_block(filters, x)\n",
    "      x = residual_block(filters, x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def sublayer(next_layer):\n",
    "  def inner(x, labels, filters, time_embedding):\n",
    "    return unet_layer(filters, next_layer)(x, labels, time_embedding)\n",
    "  return inner\n",
    "\n",
    "def positional_encoding2d():\n",
    "  def inner(inputs):\n",
    "    _, w, h, c = inputs.shape\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    x = tf.range(start=0, limit=w, delta=1)\n",
    "    x = x / w\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    assert x.shape == (1, w)\n",
    "    x = tf.tile(x, multiples=[h, 1])\n",
    "    assert x.shape == (w, h)\n",
    "    x = tf.reshape(x, shape=(w, h, 1))\n",
    "\n",
    "    y = tf.range(start=0, limit=h, delta=1)\n",
    "    y = y / h\n",
    "    y = tf.expand_dims(y, axis=1)\n",
    "    assert y.shape == (h, 1)\n",
    "    y = tf.tile(y, [1, w])\n",
    "    assert y.shape == (w, h)\n",
    "    y = tf.reshape(y, shape=(w, h, 1))\n",
    "\n",
    "    indexes = tf.concat([x, y], axis=-1)\n",
    "    assert indexes.shape == (w, h, 2)\n",
    "\n",
    "    indexes = tf.expand_dims(indexes, axis=0)\n",
    "    indexes = tf.tile(indexes, [batch_size, 1, 1, 1])\n",
    "\n",
    "    return layers.Conv2D(c, kernel_size=1, strides=1, padding=\"same\")(indexes)\n",
    "    # Todo: the sinusoidal way from All you need is attention\n",
    "  return inner\n",
    "\n",
    "def self_attention(num_heads=3, key_dim=64):\n",
    "  def inner(x):\n",
    "    # add positional encoding?\n",
    "    pos = positional_encoding2d()(x)\n",
    "    x = layers.add([x, pos])\n",
    "    x = layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(x, x, x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def cross_attention(num_heads=1, key_dim=64):\n",
    "  def inner(qk, v):\n",
    "    pos_qk = positional_encoding2d()(qk)\n",
    "    pos_v = positional_encoding2d()(v)\n",
    "    qk = layers.add([qk, pos_qk])\n",
    "    v = layers.add([v, pos_v])\n",
    "    return layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(qk, v, qk)\n",
    "  return inner\n",
    "\n",
    "def get_model(num_labels, filters):\n",
    "  if os.path.exists(model_path):\n",
    "    return keras.models.load_model(model_path, custom_objects={\n",
    "      \"Swish\": Swish,\n",
    "      \"GroupNormalization\": GroupNormalization\n",
    "    })\n",
    "\n",
    "  w, h, c = latent_dim_size\n",
    "  image_input = keras.Input(shape=(w, h, c), name=\"images\")\n",
    "  t_input = keras.Input(shape=(1,), name=\"t_input\")\n",
    "  label_input = keras.Input(shape=(num_labels,), name=\"labels\")\n",
    "  \n",
    "  time = layers.Dense(units=16)(t_input)\n",
    "  cond = layers.Dense(units=16)(label_input)\n",
    "\n",
    "  sublayers = sublayer(None)\n",
    "  for i in range(num_layers):\n",
    "    sublayers = sublayer(sublayers)\n",
    "\n",
    "  x = unet_layer(filters, sublayers)(image_input, cond, time)\n",
    "  output = layers.Conv2D(filters=embedding_dim, kernel_size=3, padding=\"same\")(x)\n",
    "  model = keras.Model([image_input, label_input, t_input], output)\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "class DiffusionMonitor(keras.callbacks.Callback):\n",
    "  def __init__(self, decoder, num_img=3):\n",
    "    self.num_img = num_img\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if epoch % 5 != 0:\n",
    "      return\n",
    "    generated_images = self.model.sample(self.num_img)\n",
    "    generated_images = self.decoder(generated_images)\n",
    "    if not os.path.exists(img_output_path):\n",
    "      os.mkdir(img_output_path)\n",
    "\n",
    "    for i in range(self.num_img):\n",
    "      img = keras.utils.array_to_img(generated_images[i])\n",
    "      img.save(os.path.join(img_output_path, f\"generated_{epoch:03d}_{i}.png\"))\n",
    "\n",
    "class Save(keras.callbacks.Callback):\n",
    "  def __init__(self, model_path):\n",
    "    self.model_path = model_path\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.model.model.save(self.model_path)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "model = get_model(num_label_columns, filters)\n",
    "model.summary()\n",
    "\n",
    "diffusion = Diffusion(model, steps, variance_schedule)\n",
    "diffusion.compile(optimizer=\"rmsprop\", run_eagerly=runeager)\n",
    "\n",
    "callbacks = [\n",
    "  DiffusionMonitor(decoder),\n",
    "  Save(model_path)\n",
    "]\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Load prepared dataset\n",
    "\n",
    "# %%\n",
    "dataset = tf.data.Dataset.load(ds_path, compression='GZIP')\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "# %%\n",
    "\n",
    "diffusion.fit(dataset, callbacks=callbacks, epochs=epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32f2fe102a4f10662d8c13f75131e1ba377b7194060421a642fdea27c55fc65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
