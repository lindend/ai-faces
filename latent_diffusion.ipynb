{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_length = 2048\n",
    "embedding_dim = 4\n",
    "image_size = (224, 224)\n",
    "latent_dim_size = (224 // 4, 224 // 4, embedding_dim)\n",
    "beta = 0.25\n",
    "runeager = False\n",
    "small_dataset = True\n",
    "ds_size = 20048\n",
    "batch_size = 32\n",
    "test_size = 5\n",
    "epochs = 1000\n",
    "num_layers = 2\n",
    "# Number of columns in list_attr_celeba.txt\n",
    "num_label_columns = 40\n",
    "\n",
    "steps = 500\n",
    "variance_schedule_start = 0.0001\n",
    "variance_schedule_end = 0.02\n",
    "variance_schedule = [i * (variance_schedule_end - variance_schedule_start) / steps + variance_schedule_start for i in range(steps)]\n",
    "\n",
    "filters = 32\n",
    "\n",
    "version = 1\n",
    "\n",
    "root = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in colab\n"
     ]
    }
   ],
   "source": [
    "try: #If running in colab \n",
    "  import google.colab\n",
    "  !pip install tensorflow==2.10.0\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  !cp -r drive/MyDrive/datasets .\n",
    "  # !cp drive/MyDrive/ai-faces.zip .\n",
    "  # !unzip ai-faces.zip\n",
    "\n",
    "  # Override parameters when running in colab\n",
    "  root = \"drive/MyDrive/\"\n",
    "  small_dataset = True\n",
    "  runeager = False\n",
    "  filters = 32\n",
    "except:\n",
    "  print('Not running in colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_output_path = f\"{root}generated-latent-diffusion-v{version}\"\n",
    "model_path = f\"{root}models/latent_diffusion_faces_v{version}.keras\"\n",
    "    \n",
    "ae_model_path = f\"{root}models/vqgan_faces_v8.keras\"\n",
    "ds_suffix = '_small' if small_dataset else ''\n",
    "ds_path = f\"datasets/vqgan_faces_v8{ds_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.losses\n",
    "import keras.optimizers\n",
    "import keras.layers as layers\n",
    "import keras.losses as losses\n",
    "import keras.callbacks as callbacks\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and decoder\n",
    "Encoder should not include vector quantization, however the decoder should. conv2d_12 is the last layer before the quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(layers.Layer):\n",
    "  def call(self, x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "class GroupNormalization(layers.Layer):\n",
    "  def __init__(self, num_groups = 32, epsilon=1e-7, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.num_groups = num_groups\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    (_, _, _, C) = input_shape\n",
    "    self.channel_weights = self.add_weight(\"channel_weights\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n",
    "    self.channel_biases = self.add_weight(\"channel_biases\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    (_, W, H, C) = x.shape\n",
    "    B = tf.shape(x)[0]\n",
    "    x = tf.reshape(x, shape=(B, W, H, self.num_groups, C // self.num_groups))\n",
    "    mean, var = tf.nn.moments(x, [1, 2, 4], keepdims=True)\n",
    "    x = (x - mean) / tf.sqrt(var + self.epsilon)\n",
    "    x = tf.reshape(x, shape=(B, W, H, C))\n",
    "    x = x * self.channel_weights + self.channel_biases\n",
    "    return x\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(GroupNormalization, self).get_config()\n",
    "    config.update({\n",
    "      \"num_groups\": self.num_groups,\n",
    "      \"epsilon\": self.epsilon\n",
    "    })\n",
    "    return config\n",
    "\n",
    "class VectorQuantization(layers.Layer):\n",
    "  def __init__(self, embedding_length, embedding_dim, beta=0.25, **kwargs):\n",
    "    super(VectorQuantization, self).__init__(**kwargs)\n",
    "    self.embedding_length = embedding_length\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.beta = beta\n",
    "    self.embedding = self.add_weight(\"embedding\",\n",
    "      shape=(embedding_length, embedding_dim),\n",
    "      initializer=tf.random_uniform_initializer(-1.0, 1.0), \n",
    "      trainable=True)\n",
    "\n",
    "  def call(self, input):\n",
    "    (_, w, h, c) = input.shape\n",
    "    B = tf.shape(input)[0]\n",
    "    flat = tf.reshape(input, shape=(B * w * h, c))\n",
    "    flat = tf.tile(flat, [1, self.embedding_length])\n",
    "    flat = tf.reshape(flat, shape=(B * w * h, self.embedding_length, c))\n",
    "    diff = tf.pow(flat - self.embedding, 2)\n",
    "    diff = tf.reduce_sum(diff, axis=-1)\n",
    "    embedding_indexes = tf.argmin(diff, axis=-1)\n",
    "    embedding_indexes = tf.reshape(embedding_indexes, shape=(B, w, h))\n",
    "    quantized_vectors = tf.gather(self.embedding, embedding_indexes)\n",
    "\n",
    "    embedding_loss = tf.reduce_mean((tf.stop_gradient(input) - quantized_vectors) ** 2)\n",
    "    encoding_loss = tf.reduce_mean((input - tf.stop_gradient(quantized_vectors)) ** 2)\n",
    "    self.add_loss(embedding_loss + self.beta * encoding_loss)\n",
    "\n",
    "    # Straight through estimator\n",
    "    quantized_vectors = input + tf.stop_gradient(quantized_vectors - input)\n",
    "    return quantized_vectors\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(VectorQuantization, self).get_config()\n",
    "    config.update({\n",
    "      \"embedding_length\": self.embedding_length,\n",
    "      \"embedding_dim\": self.embedding_dim,\n",
    "      \"beta\": self.beta\n",
    "    })\n",
    "    return config\n",
    "\n",
    "custom_objects = {\n",
    "  \"Swish\": Swish,\n",
    "  \"GroupNormalization\": GroupNormalization,\n",
    "  \"VectorQuantization\": VectorQuantization\n",
    "}\n",
    "\n",
    "autoencoder = keras.models.load_model(ae_model_path, custom_objects, compile=False)\n",
    "encoder = keras.models.Model(autoencoder.input, autoencoder.get_layer(\"conv2d_12\").output, name=\"encoder\")\n",
    "decoder = keras.models.Model(autoencoder.get_layer(\"vector_quantization\").input, autoencoder.output, name=\"decoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset, write it to file to optimize performance runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ds_path):\n",
    "  fname = os.path.join(\"list_attr_celeba.txt\")\n",
    "\n",
    "  with open(fname) as f:\n",
    "    data = f.read()\n",
    "\n",
    "  lines = data.split(\"\\n\")\n",
    "  header = lines[1].split()\n",
    "  lines = lines[2:-1]\n",
    "  raw_data = np.zeros((len(lines), len(header)))\n",
    "  for i, line in enumerate(lines):\n",
    "    line_data = [int(x) for x in line.split()[1:]]\n",
    "    raw_data[i, :] = line_data[:]\n",
    "\n",
    "  label_dataset = tf.data.Dataset.from_tensor_slices(raw_data).batch(batch_size)\n",
    "  image_dataset = keras.utils.image_dataset_from_directory(\n",
    "    f\"img_align_celeba{ds_suffix}\",\n",
    "    label_mode=None,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    smart_resize=True,\n",
    "    shuffle=False)\n",
    "\n",
    "  image_dataset = (image_dataset\n",
    "    .map(lambda x: x / (255. / 2) - 1., num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .map(lambda x: encoder(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "  )\n",
    "\n",
    "  dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "  dataset = dataset.take(ds_size)\n",
    "  dataset.save(ds_path, compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Diffusion(keras.models.Model):\n",
    "  def __init__(self, model,  num_steps, variance_schedule, **kwargs):\n",
    "    super().__init__(kwargs)\n",
    "    self.num_steps = num_steps\n",
    "    self.loss_tracker = keras.metrics.Mean(\"loss\")\n",
    "    self.model = model\n",
    "    self.loss_fn = keras.losses.MSE\n",
    "    self.variance_schedule = variance_schedule\n",
    "    self.alpha = [1 - b for b in variance_schedule]\n",
    "    self.alpha_accumulated = []\n",
    "    total = 1.\n",
    "    for a in self.alpha:\n",
    "      total *= a\n",
    "      self.alpha_accumulated.append(total)\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [self.loss_tracker]\n",
    "\n",
    "  def train_step(self, input):\n",
    "    real_images, labels = input\n",
    "    _, width, height, channels = real_images.shape\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    labels = labels[:batch_size]\n",
    "    t = tf.random.uniform(shape=(batch_size,), minval=0, maxval=self.num_steps, dtype=tf.int32)\n",
    "    t_input = t / self.num_steps\n",
    "    input_shape = (batch_size, width, height, channels)\n",
    "    noise = tf.random.normal(shape=input_shape)\n",
    "    alpha_t = tf.gather(self.alpha_accumulated, t)\n",
    "    alpha_t = tf.reshape(alpha_t, shape=(batch_size, 1, 1, 1))\n",
    "    noise_variance = tf.sqrt(1 - alpha_t) * noise\n",
    "    img_median = tf.sqrt(alpha_t) * real_images\n",
    "    noisy_input = img_median + noise_variance\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      predicted_noise = self.model([noisy_input, labels, t_input])\n",
    "      noise_loss = self.loss_fn(noise, predicted_noise)\n",
    "    grads = tape.gradient(noise_loss, self.model.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "    self.loss_tracker.update_state(noise_loss)\n",
    "\n",
    "    return {\n",
    "      \"loss\": self.loss_tracker.result()\n",
    "    }\n",
    "\n",
    "  def sample(self, num_images, labels=None):\n",
    "    result_shape = (num_images, *latent_dim_size)\n",
    "    result = tf.random.normal(shape=result_shape)\n",
    "    for t in reversed(range(1, self.num_steps)):\n",
    "      if t > 1:\n",
    "        z = tf.random.normal(shape=result_shape)\n",
    "      else:\n",
    "        z = tf.zeros(shape=result_shape)\n",
    "      \n",
    "      alpha = self.alpha[t]\n",
    "      alpha_t = self.alpha_accumulated[t]\n",
    "      t_input = tf.constant(t / self.num_steps, dtype=tf.float32)\n",
    "      t_input = tf.broadcast_to(t_input, shape=(num_images,))\n",
    "      # Todo: make 40 not hardcoded\n",
    "      if labels is None:\n",
    "        labels = tf.random.uniform(shape=(num_images, 40), minval=0, maxval=1, dtype=tf.int32)\n",
    "      # Convert to -1 or 1\n",
    "      labels = labels * 2 - 1\n",
    "      predicted_noise = self.model([result, labels, t_input])\n",
    "      noise_factor = (1 - alpha) / tf.sqrt(1 - alpha_t)\n",
    "      sigma = tf.sqrt(self.variance_schedule[t])\n",
    "      result = (1 / tf.sqrt(alpha)) * (result - noise_factor * predicted_noise) + sigma * z\n",
    "\n",
    "    return result\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters):\n",
    "  def inner(x):\n",
    "    x = layers.Conv2D(filters=filters, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def time_embedding_proj(shape, idx, dense_dim=8):\n",
    "  def inner(x):\n",
    "    b, w, h, c = shape\n",
    "    x = layers.Dense(units=dense_dim, activation=\"relu\", name=f\"time_embedding_{idx}_0\")(x)\n",
    "    x = layers.Dense(units=w * h, name=f\"time_embedding_{idx}_1\")(x)\n",
    "    x = layers.Reshape(target_shape=(w, h, 1), name=f\"time_embedding_reshape_{idx}\")(x)\n",
    "    x = layers.Conv2D(filters=c, kernel_size=1, activation=\"relu\", name=f\"time_embedding_{idx}_2\")(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def downsample():\n",
    "  def inner(x):\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def upsample(filters):\n",
    "  def inner(x):\n",
    "    x = layers.Conv2DTranspose(kernel_size=4, strides=2, filters=filters, padding=\"same\")(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def dropout(rate):\n",
    "  def inner(x):\n",
    "    x = layers.SpatialDropout2D(rate)(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def unet_layer(filters, next_layer):\n",
    "  def inner(x, labels, time_embedding, idx=0):\n",
    "    labels = layers.Dense(units=8, name=f\"labels_proj_{idx}\")(labels)\n",
    "    _, w, h, c = x.shape\n",
    "    mapped_labels = layers.Dense(units=w * h, name=f\"map_labels_{idx}\")(labels)\n",
    "    mapped_labels = layers.Reshape(target_shape=(w, h, 1))(mapped_labels)\n",
    "    mapped_labels = layers.Conv2D(filters=c, kernel_size=1, name=f\"conv_labels_{idx}\")(mapped_labels)\n",
    "    x = layers.add([x, mapped_labels])\n",
    "    x = conv_block(filters)(x)\n",
    "    # In bottom layer, do self attention\n",
    "    if next_layer is None:\n",
    "      x = self_attention(3)(x)\n",
    "    x = dropout(0.2)(x)\n",
    "    x = conv_block(filters)(x)\n",
    "    residual = x\n",
    "    if next_layer is not None:\n",
    "      x = downsample()(x)\n",
    "      x = next_layer(x, labels, filters * 2, time_embedding, idx + 1)\n",
    "      x = upsample(filters)(x)\n",
    "      time = time_embedding_proj(x.shape, idx)(time_embedding)\n",
    "      x = layers.add([residual, x, time])\n",
    "      x = conv_block(filters)(x)\n",
    "      x = conv_block(filters)(x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def sublayer(next_layer):\n",
    "  def inner(x, labels, filters, time_embedding, idx):\n",
    "    return unet_layer(filters, next_layer)(x, labels, time_embedding, idx)\n",
    "  return inner\n",
    "\n",
    "def positional_encoding2d():\n",
    "  def inner(inputs):\n",
    "    _, w, h, c = inputs.shape\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    x = tf.range(start=0, limit=w, delta=1)\n",
    "    x = x / w\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    assert x.shape == (1, w)\n",
    "    x = tf.tile(x, multiples=[h, 1])\n",
    "    assert x.shape == (w, h)\n",
    "    x = tf.reshape(x, shape=(w, h, 1))\n",
    "\n",
    "    y = tf.range(start=0, limit=h, delta=1)\n",
    "    y = y / h\n",
    "    y = tf.expand_dims(y, axis=1)\n",
    "    assert y.shape == (h, 1)\n",
    "    y = tf.tile(y, [1, w])\n",
    "    assert y.shape == (w, h)\n",
    "    y = tf.reshape(y, shape=(w, h, 1))\n",
    "\n",
    "    indexes = tf.concat([x, y], axis=-1)\n",
    "    assert indexes.shape == (w, h, 2)\n",
    "\n",
    "    indexes = tf.expand_dims(indexes, axis=0)\n",
    "    indexes = tf.tile(indexes, [batch_size, 1, 1, 1])\n",
    "\n",
    "    return layers.Conv2D(c, kernel_size=1, strides=1, padding=\"same\")(indexes)\n",
    "    # Todo: the sinusoidal way from All you need is attention\n",
    "  return inner\n",
    "\n",
    "def self_attention(num_heads=1, key_dim=64):\n",
    "  def inner(x):\n",
    "    # add positional encoding?\n",
    "    pos = positional_encoding2d()(x)\n",
    "    x = layers.add([x, pos])\n",
    "    x = layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(x, x, x)\n",
    "    return x\n",
    "  return inner\n",
    "\n",
    "def cross_attention(num_heads=1, key_dim=64):\n",
    "  def inner(qk, v):\n",
    "    pos_qk = positional_encoding2d()(qk)\n",
    "    pos_v = positional_encoding2d()(v)\n",
    "    qk = layers.add([qk, pos_qk])\n",
    "    v = layers.add([v, pos_v])\n",
    "    return layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(qk, v, qk)\n",
    "  return inner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(num_labels, filters):\n",
    "  if os.path.exists(model_path):\n",
    "    return keras.models.load_model(model_path)\n",
    "\n",
    "  w, h, c = latent_dim_size\n",
    "  image_input = keras.Input(shape=(w, h, c), name=\"images\")\n",
    "  t_input = keras.Input(shape=(1,), name=\"t_input\")\n",
    "  label_input = keras.Input(shape=(num_labels,), name=\"labels\")\n",
    "  sublayers = sublayer(None)\n",
    "  for i in range(num_layers):\n",
    "    sublayers = sublayer(sublayers)\n",
    "\n",
    "  x = unet_layer(filters, sublayers)(image_input, label_input, t_input)\n",
    "  output = layers.Conv2D(filters=embedding_dim, kernel_size=3, padding=\"same\")(x)\n",
    "  model = keras.Model([image_input, label_input, t_input], output)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionMonitor(keras.callbacks.Callback):\n",
    "  def __init__(self, decoder, num_img=3):\n",
    "    self.num_img = num_img\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if epoch % 10 != 0:\n",
    "      return\n",
    "    generated_images = self.model.sample(self.num_img)\n",
    "    generated_images = self.decoder(generated_images)\n",
    "    if not os.path.exists(img_output_path):\n",
    "      os.mkdir(img_output_path)\n",
    "\n",
    "    for i in range(self.num_img):\n",
    "      img = keras.utils.array_to_img(generated_images[i])\n",
    "      img.save(os.path.join(img_output_path, f\"generated_{epoch:03d}_{i}.png\"))\n",
    "\n",
    "class Save(keras.callbacks.Callback):\n",
    "  def __init__(self, model_path):\n",
    "    self.model_path = model_path\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.model.model.save(self.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " labels (InputLayer)            [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " labels_proj_0 (Dense)          (None, 8)            328         ['labels[0][0]']                 \n",
      "                                                                                                  \n",
      " map_labels_0 (Dense)           (None, 3136)         28224       ['labels_proj_0[0][0]']          \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 56, 56, 1)    0           ['map_labels_0[0][0]']           \n",
      "                                                                                                  \n",
      " images (InputLayer)            [(None, 56, 56, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " conv_labels_0 (Conv2D)         (None, 56, 56, 4)    8           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 56, 56, 4)    0           ['images[0][0]',                 \n",
      "                                                                  'conv_labels_0[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 56, 56, 32)   1184        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " labels_proj_1 (Dense)          (None, 8)            72          ['labels_proj_0[0][0]']          \n",
      "                                                                                                  \n",
      " spatial_dropout2d (SpatialDrop  (None, 56, 56, 32)  0           ['conv2d[0][0]']                 \n",
      " out2D)                                                                                           \n",
      "                                                                                                  \n",
      " map_labels_1 (Dense)           (None, 784)          7056        ['labels_proj_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 56, 56, 32)   9248        ['spatial_dropout2d[0][0]']      \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 28, 28, 1)    0           ['map_labels_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 28, 28, 32)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv_labels_1 (Conv2D)         (None, 28, 28, 32)   64          ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 32)   0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv_labels_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 28, 64)   18496       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " labels_proj_2 (Dense)          (None, 8)            72          ['labels_proj_1[0][0]']          \n",
      "                                                                                                  \n",
      " spatial_dropout2d_1 (SpatialDr  (None, 28, 28, 64)  0           ['conv2d_2[0][0]']               \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " map_labels_2 (Dense)           (None, 196)          1764        ['labels_proj_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 28, 28, 64)   36928       ['spatial_dropout2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 14, 14, 1)    0           ['map_labels_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv_labels_2 (Conv2D)         (None, 14, 14, 64)   128         ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 14, 14, 64)   0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv_labels_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 14, 14, 128)  73856       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " labels_proj_3 (Dense)          (None, 8)            72          ['labels_proj_2[0][0]']          \n",
      "                                                                                                  \n",
      " spatial_dropout2d_2 (SpatialDr  (None, 14, 14, 128)  0          ['conv2d_4[0][0]']               \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " map_labels_3 (Dense)           (None, 49)           441         ['labels_proj_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 14, 14, 128)  147584      ['spatial_dropout2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 7, 7, 1)      0           ['map_labels_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 128)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv_labels_3 (Conv2D)         (None, 7, 7, 128)    256         ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 7, 7, 128)    0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv_labels_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 7, 7, 256)    295168      ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_2 (TFOpLamb  (4,)                0           ['conv2d_6[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  ()                  0           ['tf.compat.v1.shape_2[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.tile_2 (TFOpLambda)         (None, 7, 7, 2)      0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 7, 7, 256)    768         ['tf.tile_2[0][0]']              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 7, 7, 256)    0           ['conv2d_6[0][0]',               \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " t_input (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 7, 7, 256)   197440      ['add_4[0][0]',                  \n",
      " dAttention)                                                      'add_4[0][0]',                  \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " time_embedding_2_0 (Dense)     (None, 8)            16          ['t_input[0][0]']                \n",
      "                                                                                                  \n",
      " spatial_dropout2d_3 (SpatialDr  (None, 7, 7, 256)   0           ['multi_head_attention[0][0]']   \n",
      " opout2D)                                                                                         \n",
      "                                                                                                  \n",
      " time_embedding_2_1 (Dense)     (None, 196)          1764        ['time_embedding_2_0[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 7, 7, 256)    590080      ['spatial_dropout2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " time_embedding_reshape_2 (Resh  (None, 14, 14, 1)   0           ['time_embedding_2_1[0][0]']     \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 14, 14, 128)  524416     ['conv2d_8[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " time_embedding_2_2 (Conv2D)    (None, 14, 14, 128)  256         ['time_embedding_reshape_2[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 14, 14, 128)  0           ['conv2d_5[0][0]',               \n",
      "                                                                  'conv2d_transpose[0][0]',       \n",
      "                                                                  'time_embedding_2_2[0][0]']     \n",
      "                                                                                                  \n",
      " time_embedding_1_0 (Dense)     (None, 8)            16          ['t_input[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 14, 14, 128)  147584      ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " time_embedding_1_1 (Dense)     (None, 784)          7056        ['time_embedding_1_0[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 14, 128)  147584      ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " time_embedding_reshape_1 (Resh  (None, 28, 28, 1)   0           ['time_embedding_1_1[0][0]']     \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 28, 28, 64)  131136      ['conv2d_10[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " time_embedding_1_2 (Conv2D)    (None, 28, 28, 64)   128         ['time_embedding_reshape_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 28, 28, 64)   0           ['conv2d_3[0][0]',               \n",
      "                                                                  'conv2d_transpose_1[0][0]',     \n",
      "                                                                  'time_embedding_1_2[0][0]']     \n",
      "                                                                                                  \n",
      " time_embedding_0_0 (Dense)     (None, 8)            16          ['t_input[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 28, 28, 64)   36928       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " time_embedding_0_1 (Dense)     (None, 3136)         28224       ['time_embedding_0_0[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 28, 28, 64)   36928       ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " time_embedding_reshape_0 (Resh  (None, 56, 56, 1)   0           ['time_embedding_0_1[0][0]']     \n",
      " ape)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 56, 56, 32)  32800       ['conv2d_12[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " time_embedding_0_2 (Conv2D)    (None, 56, 56, 32)   64          ['time_embedding_reshape_0[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 56, 56, 32)   0           ['conv2d_1[0][0]',               \n",
      "                                                                  'conv2d_transpose_2[0][0]',     \n",
      "                                                                  'time_embedding_0_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 56, 56, 32)   9248        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 56, 56, 32)   9248        ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 56, 56, 4)    1156        ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,523,805\n",
      "Trainable params: 2,523,805\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_model(num_label_columns, filters)\n",
    "model.summary()\n",
    "\n",
    "diffusion = Diffusion(model, steps, variance_schedule)\n",
    "diffusion.compile(optimizer=\"rmsprop\", run_eagerly=runeager)\n",
    "\n",
    "callbacks = [\n",
    "  DiffusionMonitor(decoder),\n",
    "  Save(model_path)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.load(ds_path, compression='GZIP')\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.5019WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 30s 627ms/step - loss: 0.5019\n",
      "Epoch 2/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.3468WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 23s 575ms/step - loss: 0.3459\n",
      "Epoch 3/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.3316WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "41/41 [==============================] - 23s 578ms/step - loss: 0.3319\n",
      "Epoch 4/1000\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.3200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dev\\ai-faces\\latent_diffusion.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m diffusion\u001b[39m.\u001b[39;49mfit(dataset, callbacks\u001b[39m=\u001b[39;49mcallbacks, epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1624\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m     }\n\u001b[0;32m   1622\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1624\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[0;32m   1625\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[0;32m   1626\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    446\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m--> 448\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "\u001b[1;32mc:\\Dev\\ai-faces\\latent_diffusion.ipynb Cell 17\u001b[0m in \u001b[0;36mDiffusionMonitor.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   generated_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msample(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   generated_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(generated_images)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(img_output_path):\n",
      "\u001b[1;32mc:\\Dev\\ai-faces\\latent_diffusion.ipynb Cell 17\u001b[0m in \u001b[0;36mDiffusion.sample\u001b[1;34m(self, num_images, labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Convert to -1 or 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m labels \u001b[39m=\u001b[39m labels \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m predicted_noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel([result, labels, t_input])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m noise_factor \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha) \u001b[39m/\u001b[39m tf\u001b[39m.\u001b[39msqrt(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha_t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/ai-faces/latent_diffusion.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m sigma \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_schedule[t])\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:510\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m     \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:667\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 667\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    671\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    672\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py:137\u001b[0m, in \u001b[0;36mReshape.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m--> 137\u001b[0m     result \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(inputs, (tf\u001b[39m.\u001b[39;49mshape(inputs)[\u001b[39m0\u001b[39m],) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_shape)\n\u001b[0;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    139\u001b[0m         \u001b[39m# Set the static shape for the result since it might lost during\u001b[39;00m\n\u001b[0;32m    140\u001b[0m         \u001b[39m# array_ops reshape, eg, some `None` dim in the result could be\u001b[39;00m\n\u001b[0;32m    141\u001b[0m         \u001b[39m# inferred.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m         result\u001b[39m.\u001b[39mset_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_output_shape(inputs\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:629\u001b[0m, in \u001b[0;36mshape_v2\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    581\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m    582\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshape_v2\u001b[39m(\u001b[39minput\u001b[39m, out_type\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    583\u001b[0m   \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m    584\u001b[0m   \u001b[39m\"\"\"Returns a tensor containing the shape of the input tensor.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \n\u001b[0;32m    586\u001b[0m \u001b[39m  See also `tf.size`, `tf.rank`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39m    A `Tensor` of type `out_type`.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m   \u001b[39mreturn\u001b[39;00m shape(\u001b[39minput\u001b[39;49m, name, out_type)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:656\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    633\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshape\u001b[39m(\u001b[39minput\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out_type\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint32):\n\u001b[0;32m    635\u001b[0m   \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m    636\u001b[0m   \u001b[39m\"\"\"Returns the shape of a tensor.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \n\u001b[0;32m    638\u001b[0m \u001b[39m  This operation returns a 1-D integer tensor representing the shape of `input`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[39m    A `Tensor` of type `out_type`.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 656\u001b[0m   \u001b[39mreturn\u001b[39;00m shape_internal(\u001b[39minput\u001b[39;49m, name, optimize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, out_type\u001b[39m=\u001b[39;49mout_type)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:697\u001b[0m, in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out_type:\n\u001b[0;32m    696\u001b[0m   out_type \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mint32\n\u001b[1;32m--> 697\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mshape(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname, out_type\u001b[39m=\u001b[39;49mout_type)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:9355\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9353\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   9354\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 9355\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   9356\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mShape\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mout_type\u001b[39;49m\u001b[39m\"\u001b[39;49m, out_type)\n\u001b[0;32m   9357\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   9358\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "diffusion.fit(dataset, callbacks=callbacks, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32f2fe102a4f10662d8c13f75131e1ba377b7194060421a642fdea27c55fc65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
