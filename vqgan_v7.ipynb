{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19178,"status":"ok","timestamp":1669645076655,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"mp-QDum_NKuy","outputId":"820dbd71-ef10-4019-a696-1ae54f012ae2"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# !cp drive/MyDrive/ai-faces.zip .\n","# !unzip ai-faces.zip"]},{"cell_type":"markdown","metadata":{"id":"pVvQVaUDJyD7"},"source":["Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2585,"status":"ok","timestamp":1669645057482,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"Cw7BtwlVJyD9"},"outputs":[],"source":["import os\n","import keras\n","import keras.losses\n","import keras.optimizers\n","import keras.layers as layers\n","import keras.losses as losses\n","import keras.callbacks as callbacks\n","import keras.backend as K\n","import tensorflow as tf\n","import numpy as np\n","from keras.applications.efficientnet_v2 import EfficientNetV2B0\n","import shutil"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669645057482,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"wqtHSpn7JyD-"},"outputs":[],"source":["image_size = (224, 224)\n","embedding_length = 2048\n","embedding_dim = 4\n","beta = 0.25\n","runeager = False\n","small_dataset = False\n","ds_size = 20048\n","batch_size = 32\n","test_size = 5\n","epochs = 1000\n","eg_learning_rate = 0.0001\n","d_learning_rate = 0.0001\n","discriminator_weight = 1.0\n","\n","filters = 32\n","\n","version = 8\n","\n","root = \"\"#\"drive/MyDrive/\"\n","\n","img_output_path = f\"{root}generated-vqgan-v{version}\"\n","log_dir = \"logs/vqgan\"\n","def model_path(model):\n","  return f\"{root}models/vqgan_faces_{model}_v{version}.h5\"\n","\n","def optimizer_path(optimizer):\n","  return f\"{root}models/vqgan_faces_optimizer_{optimizer}_v{version}.npz\"\n","  \n","if os.path.exists(os.path.join(log_dir, \"train\")):\n","  shutil.rmtree(os.path.join(log_dir, \"train\"))"]},{"cell_type":"markdown","metadata":{"id":"0wJzmtZXJyD_"},"source":["Perceptual loss"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6099,"status":"ok","timestamp":1669645142028,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"Z-rMYMrsJyD_","outputId":"3f47641d-e2e0-4764-e2a6-79ecf72e46bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"perceptual_loss\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," rescaling (Rescaling)          (None, 224, 224, 3)  0           ['input_1[0][0]']                \n","                                                                                                  \n"," normalization (Normalization)  (None, 224, 224, 3)  0           ['rescaling[0][0]']              \n","                                                                                                  \n"," stem_conv (Conv2D)             (None, 112, 112, 32  864         ['normalization[0][0]']          \n","                                )                                                                 \n","                                                                                                  \n"," stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n","                                )                                                                 \n","                                                                                                  \n"," stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," block1a_project_conv (Conv2D)  (None, 112, 112, 16  4608        ['stem_activation[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n"," lization)                      )                                                                 \n","                                                                                                  \n"," block1a_project_activation (Ac  (None, 112, 112, 16  0          ['block1a_project_bn[0][0]']     \n"," tivation)                      )                                                                 \n","                                                                                                  \n"," block2a_expand_conv (Conv2D)   (None, 56, 56, 64)   9216        ['block1a_project_activation[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," block2a_expand_bn (BatchNormal  (None, 56, 56, 64)  256         ['block2a_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block2a_expand_activation (Act  (None, 56, 56, 64)  0           ['block2a_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block2a_project_conv (Conv2D)  (None, 56, 56, 32)   2048        ['block2a_expand_activation[0][0]\n","                                                                 ']                               \n","                                                                                                  \n"," block2a_project_bn (BatchNorma  (None, 56, 56, 32)  128         ['block2a_project_conv[0][0]']   \n"," lization)                                                                                        \n","                                                                                                  \n"," block2b_expand_conv (Conv2D)   (None, 56, 56, 128)  36864       ['block2a_project_bn[0][0]']     \n","                                                                                                  \n"," block2b_expand_bn (BatchNormal  (None, 56, 56, 128)  512        ['block2b_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block2b_expand_activation (Act  (None, 56, 56, 128)  0          ['block2b_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block2b_project_conv (Conv2D)  (None, 56, 56, 32)   4096        ['block2b_expand_activation[0][0]\n","                                                                 ']                               \n","                                                                                                  \n"," block2b_project_bn (BatchNorma  (None, 56, 56, 32)  128         ['block2b_project_conv[0][0]']   \n"," lization)                                                                                        \n","                                                                                                  \n"," block2b_drop (Dropout)         (None, 56, 56, 32)   0           ['block2b_project_bn[0][0]']     \n","                                                                                                  \n"," block2b_add (Add)              (None, 56, 56, 32)   0           ['block2b_drop[0][0]',           \n","                                                                  'block2a_project_bn[0][0]']     \n","                                                                                                  \n"," block3a_expand_conv (Conv2D)   (None, 28, 28, 128)  36864       ['block2b_add[0][0]']            \n","                                                                                                  \n"," block3a_expand_bn (BatchNormal  (None, 28, 28, 128)  512        ['block3a_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block3a_expand_activation (Act  (None, 28, 28, 128)  0          ['block3a_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block3a_project_conv (Conv2D)  (None, 28, 28, 48)   6144        ['block3a_expand_activation[0][0]\n","                                                                 ']                               \n","                                                                                                  \n"," block3a_project_bn (BatchNorma  (None, 28, 28, 48)  192         ['block3a_project_conv[0][0]']   \n"," lization)                                                                                        \n","                                                                                                  \n"," block3b_expand_conv (Conv2D)   (None, 28, 28, 192)  82944       ['block3a_project_bn[0][0]']     \n","                                                                                                  \n"," block3b_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block3b_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block3b_expand_activation (Act  (None, 28, 28, 192)  0          ['block3b_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block3b_project_conv (Conv2D)  (None, 28, 28, 48)   9216        ['block3b_expand_activation[0][0]\n","                                                                 ']                               \n","                                                                                                  \n"," block3b_project_bn (BatchNorma  (None, 28, 28, 48)  192         ['block3b_project_conv[0][0]']   \n"," lization)                                                                                        \n","                                                                                                  \n"," block3b_drop (Dropout)         (None, 28, 28, 48)   0           ['block3b_project_bn[0][0]']     \n","                                                                                                  \n"," block3b_add (Add)              (None, 28, 28, 48)   0           ['block3b_drop[0][0]',           \n","                                                                  'block3a_project_bn[0][0]']     \n","                                                                                                  \n"," block4a_expand_conv (Conv2D)   (None, 28, 28, 192)  9216        ['block3b_add[0][0]']            \n","                                                                                                  \n"," block4a_expand_bn (BatchNormal  (None, 28, 28, 192)  768        ['block4a_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block4a_expand_activation (Act  (None, 28, 28, 192)  0          ['block4a_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block4a_dwconv2 (DepthwiseConv  (None, 14, 14, 192)  1728       ['block4a_expand_activation[0][0]\n"," 2D)                                                             ']                               \n","                                                                                                  \n"," block4a_bn (BatchNormalization  (None, 14, 14, 192)  768        ['block4a_dwconv2[0][0]']        \n"," )                                                                                                \n","                                                                                                  \n"," block4a_activation (Activation  (None, 14, 14, 192)  0          ['block4a_bn[0][0]']             \n"," )                                                                                                \n","                                                                                                  \n"," block4a_se_squeeze (GlobalAver  (None, 192)         0           ['block4a_activation[0][0]']     \n"," agePooling2D)                                                                                    \n","                                                                                                  \n"," block4a_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block4a_se_squeeze[0][0]']     \n","                                                                                                  \n"," block4a_se_reduce (Conv2D)     (None, 1, 1, 12)     2316        ['block4a_se_reshape[0][0]']     \n","                                                                                                  \n"," block4a_se_expand (Conv2D)     (None, 1, 1, 192)    2496        ['block4a_se_reduce[0][0]']      \n","                                                                                                  \n"," block4a_se_excite (Multiply)   (None, 14, 14, 192)  0           ['block4a_activation[0][0]',     \n","                                                                  'block4a_se_expand[0][0]']      \n","                                                                                                  \n"," block4a_project_conv (Conv2D)  (None, 14, 14, 96)   18432       ['block4a_se_excite[0][0]']      \n","                                                                                                  \n"," block4a_project_bn (BatchNorma  (None, 14, 14, 96)  384         ['block4a_project_conv[0][0]']   \n"," lization)                                                                                        \n","                                                                                                  \n"," block4b_expand_conv (Conv2D)   (None, 14, 14, 384)  36864       ['block4a_project_bn[0][0]']     \n","                                                                                                  \n"," block4b_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block4b_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block4b_expand_activation (Act  (None, 14, 14, 384)  0          ['block4b_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block4b_dwconv2 (DepthwiseConv  (None, 14, 14, 384)  3456       ['block4b_expand_activation[0][0]\n"," 2D)                                                             ']                               \n","                                                                                                  \n"," block4b_bn (BatchNormalization  (None, 14, 14, 384)  1536       ['block4b_dwconv2[0][0]']        \n"," )                                                                                                \n","                                                                                                  \n"," block4b_activation (Activation  (None, 14, 14, 384)  0          ['block4b_bn[0][0]']             \n"," )                                                                                                \n","                                                                                                  \n"," block4b_se_squeeze (GlobalAver  (None, 384)         0           ['block4b_activation[0][0]']     \n"," agePooling2D)                                                                                    \n","                                                                                                  \n"," block4b_se_reshape (Reshape)   (None, 1, 1, 384)    0           ['block4b_se_squeeze[0][0]']     \n","                                                                                                  \n"," block4b_se_reduce (Conv2D)     (None, 1, 1, 24)     9240        ['block4b_se_reshape[0][0]']     \n","                                                                                                  \n"," block4b_se_expand (Conv2D)     (None, 1, 1, 384)    9600        ['block4b_se_reduce[0][0]']      \n","                                                                                                  \n"," block4b_se_excite (Multiply)   (None, 14, 14, 384)  0           ['block4b_activation[0][0]',     \n","                                                                  'block4b_se_expand[0][0]']      \n","                                                                                                  \n"," block4b_project_conv (Conv2D)  (None, 14, 14, 96)   36864       ['block4b_se_excite[0][0]']      \n","                                                                                                  \n"," block4b_project_bn (BatchNorma  (None, 14, 14, 96)  384         ['block4b_project_conv[0][0]']   \n"," lization)                                                                                        \n","                                                                                                  \n"," block4b_drop (Dropout)         (None, 14, 14, 96)   0           ['block4b_project_bn[0][0]']     \n","                                                                                                  \n"," block4b_add (Add)              (None, 14, 14, 96)   0           ['block4b_drop[0][0]',           \n","                                                                  'block4a_project_bn[0][0]']     \n","                                                                                                  \n"," block4c_expand_conv (Conv2D)   (None, 14, 14, 384)  36864       ['block4b_add[0][0]']            \n","                                                                                                  \n"," block4c_expand_bn (BatchNormal  (None, 14, 14, 384)  1536       ['block4c_expand_conv[0][0]']    \n"," ization)                                                                                         \n","                                                                                                  \n"," block4c_expand_activation (Act  (None, 14, 14, 384)  0          ['block4c_expand_bn[0][0]']      \n"," ivation)                                                                                         \n","                                                                                                  \n"," block4c_dwconv2 (DepthwiseConv  (None, 14, 14, 384)  3456       ['block4c_expand_activation[0][0]\n"," 2D)                                                             ']                               \n","                                                                                                  \n"," block4c_bn (BatchNormalization  (None, 14, 14, 384)  1536       ['block4c_dwconv2[0][0]']        \n"," )                                                                                                \n","                                                                                                  \n"," block4c_activation (Activation  (None, 14, 14, 384)  0          ['block4c_bn[0][0]']             \n"," )                                                                                                \n","                                                                                                  \n"," block4c_se_squeeze (GlobalAver  (None, 384)         0           ['block4c_activation[0][0]']     \n"," agePooling2D)                                                                                    \n","                                                                                                  \n"," block4c_se_reshape (Reshape)   (None, 1, 1, 384)    0           ['block4c_se_squeeze[0][0]']     \n","                                                                                                  \n"," block4c_se_reduce (Conv2D)     (None, 1, 1, 24)     9240        ['block4c_se_reshape[0][0]']     \n","                                                                                                  \n"," block4c_se_expand (Conv2D)     (None, 1, 1, 384)    9600        ['block4c_se_reduce[0][0]']      \n","                                                                                                  \n"," block4c_se_excite (Multiply)   (None, 14, 14, 384)  0           ['block4c_activation[0][0]',     \n","                                                                  'block4c_se_expand[0][0]']      \n","                                                                                                  \n"," block4c_project_conv (Conv2D)  (None, 14, 14, 96)   36864       ['block4c_se_excite[0][0]']      \n","                                                                                                  \n","==================================================================================================\n","Total params: 430,428\n","Trainable params: 0\n","Non-trainable params: 430,428\n","__________________________________________________________________________________________________\n"]}],"source":["perceptual_loss_net = EfficientNetV2B0(weights='imagenet')\n","input_layer = perceptual_loss_net.get_layer(\"input_1\")\n","perception_layer_names = [\n","  \"stem_conv\",\n","  \"block1a_project_conv\",\n","  \"block2a_project_conv\",\n","\n","  \"block2b_project_conv\",\n","  \"block3a_project_conv\",\n","  \"block3b_project_conv\",\n","\n","  \"block4a_project_conv\",\n","  \"block4b_project_conv\",\n","  \"block4c_project_conv\",\n","]\n","\n","perception_layers = [perceptual_loss_net.get_layer(name) for name in perception_layer_names]\n","perception_layer_outputs = [layer.output for layer in perception_layers]\n","perception_activation = keras.models.Model(input_layer.input, perception_layer_outputs, name=\"perceptual_loss\")\n","perception_activation.trainable = False\n","perception_activation.summary()\n","\n","def perceptual_loss_fn(original, generated):\n","  original_activation = perception_activation(original)\n","  generated_activation = perception_activation(generated)\n","  diffs = [tf.reduce_mean((tf.nn.l2_normalize(x, axis=-1) - tf.nn.l2_normalize(x0, axis=-1)) ** 2, axis=None) for (x, x0) in zip(original_activation, generated_activation)]\n","  loss = sum(diffs) / len(diffs)\n","  return loss + tf.reduce_mean(tf.abs(original - generated), axis=None)"]},{"cell_type":"markdown","metadata":{"id":"kD1JtXu9JyEA"},"source":["Vector Quantization"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669645142029,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"ydHnLHo8JyEB"},"outputs":[],"source":["class VectorQuantization(layers.Layer):\n","  def __init__(self, embedding_length, embedding_dim, beta=0.25, **kwargs):\n","    super(VectorQuantization, self).__init__(**kwargs)\n","    self.embedding_length = embedding_length\n","    self.embedding_dim = embedding_dim\n","    self.beta = beta\n","    self.embedding = self.add_weight(\"embedding\",\n","      shape=(embedding_length, embedding_dim),\n","      initializer=tf.random_uniform_initializer(-1.0, 1.0), \n","      trainable=True)\n","\n","  def call(self, input):\n","    (_, w, h, c) = input.shape\n","    B = tf.shape(input)[0]\n","    flat = tf.reshape(input, shape=(B * w * h, c))\n","    flat = tf.tile(flat, [1, self.embedding_length])\n","    flat = tf.reshape(flat, shape=(B * w * h, self.embedding_length, c))\n","    diff = tf.pow(flat - self.embedding, 2)\n","    diff = tf.reduce_sum(diff, axis=-1)\n","    embedding_indexes = tf.argmin(diff, axis=-1)\n","    embedding_indexes = tf.reshape(embedding_indexes, shape=(B, w, h))\n","    quantized_vectors = tf.gather(self.embedding, embedding_indexes)\n","\n","    embedding_loss = tf.reduce_mean((tf.stop_gradient(input) - quantized_vectors) ** 2)\n","    encoding_loss = tf.reduce_mean((input - tf.stop_gradient(quantized_vectors)) ** 2)\n","    self.add_loss(embedding_loss + self.beta * encoding_loss)\n","\n","    # Straight through estimator\n","    quantized_vectors = input + tf.stop_gradient(quantized_vectors - input)\n","    return quantized_vectors\n","\n","  def get_config(self):\n","    config = super(VectorQuantization, self).get_config()\n","    config.update({\n","      \"embedding_length\": self.embedding_length,\n","      \"embedding_dim\": self.embedding_dim,\n","      \"beta\": self.beta\n","    })\n","    return config"]},{"cell_type":"markdown","metadata":{"id":"bHnLXVqcJyEB"},"source":["Activation and normalization"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":354,"status":"ok","timestamp":1669645142376,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"LsFjOlY-JyEC"},"outputs":[],"source":["class Swish(layers.Layer):\n","  def call(self, x):\n","    return x * K.sigmoid(x)\n","\n","class GroupNormalization(layers.Layer):\n","  def __init__(self, num_groups = 32, epsilon=1e-7, **kwargs):\n","    super().__init__(**kwargs)\n","    self.num_groups = num_groups\n","    self.epsilon = epsilon\n","\n","  def build(self, input_shape):\n","    (_, _, _, C) = input_shape\n","    self.channel_weights = self.add_weight(\"channel_weights\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n","    self.channel_biases = self.add_weight(\"channel_biases\", shape=(1, 1, 1, C), initializer=tf.random_uniform_initializer(-1.0, 1.0), trainable=True)\n","\n","  def call(self, x):\n","    (_, W, H, C) = x.shape\n","    B = tf.shape(x)[0]\n","    x = tf.reshape(x, shape=(B, W, H, self.num_groups, C // self.num_groups))\n","    mean, var = tf.nn.moments(x, [1, 2, 4], keepdims=True)\n","    x = (x - mean) / tf.sqrt(var + self.epsilon)\n","    x = tf.reshape(x, shape=(B, W, H, C))\n","    x = x * self.channel_weights + self.channel_biases\n","    return x\n","\n","  def get_config(self):\n","    config = super(GroupNormalization, self).get_config()\n","    config.update({\n","      \"num_groups\": self.num_groups,\n","      \"epsilon\": self.epsilon\n","    })\n","    return config"]},{"cell_type":"markdown","metadata":{"id":"wkMID8ByJyED"},"source":["Load data"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8586,"status":"ok","timestamp":1669645150960,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"nOB35kxuJyEE","outputId":"9e53c00c-64de-4264-cd71-582b6b0a6607"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 202599 files belonging to 1 classes.\n"]}],"source":["dataset = keras.utils.image_dataset_from_directory(\n","  f\"img_align_celeba{'_small' if small_dataset else ''}\",\n","  label_mode=None,\n","  image_size=image_size,\n","  batch_size=batch_size,\n","  smart_resize=True,\n","  shuffle=False\n",")\n","dataset = dataset.map(lambda x: x / (255. / 2) - 1.)\n","test_ds = dataset.take(3).as_numpy_iterator()\n","dataset = dataset.skip(3)\n","\n","ds_total_size = dataset.__len__()\n","split_size = ds_total_size // 2\n","\n","eg_data = dataset.take(min(ds_size, split_size))\n","dataset = dataset.skip(min(ds_size, split_size))\n","disc_data = dataset.take(min(ds_size, split_size))\n","dataset = tf.data.Dataset.zip(((eg_data, disc_data),))\n","\n","os.makedirs(img_output_path, exist_ok=True)\n","\n","test_imgs = []\n","for batch in test_ds:\n","  test_imgs.extend(batch)\n","\n","test_imgs = test_imgs[:test_size]\n","for i, img in enumerate(test_imgs):\n","  img = keras.utils.array_to_img(img)\n","  img.save(os.path.join(img_output_path,  f\"aaref_{i}.png\"))\n","\n","test_imgs = tf.stack(test_imgs)"]},{"cell_type":"markdown","metadata":{"id":"jnLGLvLOJyEE"},"source":["VQGAN"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669645150961,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"1uVfjNMCJyEF"},"outputs":[],"source":["def sum_grads(*args):\n","  sum = None\n","  for grad in args:\n","    if sum is None:\n","      sum = grad\n","    elif grad is not None:\n","      sum += grad\n","  return sum\n","\n","\n","class VQGAN(keras.models.Model):\n","  def __init__(self, encoder, decoder, discriminator, discriminator_weight=1.4, **kwargs):\n","    super().__init__(**kwargs)\n","    self.discriminator = discriminator\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.discriminator_weight = discriminator_weight\n","    self.enc_dec = keras.Model(encoder.inputs, decoder(encoder.outputs))\n","    self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n","    self.eg_loss_metric = keras.metrics.Mean(name=\"eg_loss\")\n","    self.perceptual_loss_metric = keras.metrics.Mean(name=\"perceptual_loss\")\n","    self.decode_disc_loss_metric = keras.metrics.Mean(name=\"decode_disc_loss\")\n","    self.embedding_loss_metric = keras.metrics.Mean(name=\"embedding_loss\")\n","    self.adaptive_weight_metric = keras.metrics.Mean(name=\"adaptive_weight\")\n","\n","  @property\n","  def metrics(self):\n","    return [\n","      self.d_loss_metric, \n","      self.eg_loss_metric, \n","      self.embedding_loss_metric, \n","      self.decode_disc_loss_metric, \n","      self.perceptual_loss_metric,\n","      self.adaptive_weight_metric\n","    ]\n","\n","  def compile(self, eg_optimizer, d_optimizer, loss_fn, perceptual_loss_fn, **kwargs):\n","    super(VQGAN, self).compile(**kwargs)\n","    self.eg_optimizer = eg_optimizer\n","    self.d_optimizer = d_optimizer\n","    self.loss_fn = loss_fn\n","    self.perceptual_loss_fn = perceptual_loss_fn\n","    self.encoder.compile()\n","    self.decoder.compile()\n","    self.discriminator.compile()\n","\n","  def train_step(self, input):\n","    (eg_input, disc_input) = input[0]\n","    batch_size = tf.shape(eg_input)[0]\n","    with tf.GradientTape(persistent=True) as decoder_tape:\n","      decoded = self.enc_dec(eg_input)\n","      real_labels = tf.zeros(shape=(batch_size,))\n","      predictions = self.discriminator(decoded)\n","      perceptual_loss = self.perceptual_loss_fn(eg_input, decoded)\n","      gan_loss = self.loss_fn(real_labels, predictions)\n","      embedding_loss = sum(self.enc_dec.losses)\n","\n","    gan_grads = decoder_tape.gradient(gan_loss, self.enc_dec.trainable_weights)\n","    perceptual_loss_grads = decoder_tape.gradient(perceptual_loss, self.enc_dec.trainable_weights)\n","    embedding_grads = decoder_tape.gradient(embedding_loss, self.enc_dec.trainable_weights)\n","    adaptive_weight = tf.norm(perceptual_loss_grads[-1]) / (tf.norm(gan_grads[-1]) + 1e-6) * self.discriminator_weight\n","    grads = [sum_grads(perceptual_loss_grads[i], embedding_grads[i], gan_grads[i] * adaptive_weight if gan_grads[i] is not None else None)\n","              for i, _ in enumerate(gan_grads)]\n","    self.eg_optimizer.apply_gradients(zip(grads, self.enc_dec.trainable_weights))\n","\n","    decode_loss = perceptual_loss + embedding_loss + gan_loss * adaptive_weight\n","\n","    self.perceptual_loss_metric.update_state(perceptual_loss)\n","    self.decode_disc_loss_metric.update_state(gan_loss)\n","    self.eg_loss_metric.update_state(decode_loss)\n","    self.embedding_loss_metric.update_state(embedding_loss)\n","    self.adaptive_weight_metric.update_state(adaptive_weight)\n","\n","    with tf.GradientTape() as disc_tape:\n","      real_and_decoded_images = tf.concat([decoded, disc_input], axis=0)\n","      real_labels = tf.zeros(shape=(tf.shape(decoded)[0],))\n","      fake_labels = tf.ones(shape=(tf.shape(disc_input)[0],))\n","\n","      labels = tf.concat([fake_labels, real_labels], axis=0)\n","      labels = labels + 0.05 * tf.random.uniform(shape=tf.shape(labels))\n","      predictions = self.discriminator(real_and_decoded_images)\n","      discriminator_loss = self.loss_fn(labels, predictions)\n","\n","    disc_grads = disc_tape.gradient(discriminator_loss, self.discriminator.trainable_weights)\n","    self.d_optimizer.apply_gradients(zip(disc_grads, self.discriminator.trainable_weights))\n","\n","    self.d_loss_metric.update_state(discriminator_loss)\n","\n","    return {\n","      \"d_loss\": self.d_loss_metric.result(),\n","      \"eg_loss\": self.eg_loss_metric.result(),\n","      \"embedding_loss\": self.embedding_loss_metric.result(),\n","      \"decode_disc_loss\": self.decode_disc_loss_metric.result(),\n","      \"perceptual_loss\": self.perceptual_loss_metric.result(),\n","      \"adaptive_weight\": self.adaptive_weight_metric.result()\n","    }"]},{"cell_type":"markdown","metadata":{"id":"H0R8wpsBJyEG"},"source":["Callbacks"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1669645150961,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"9f2XxghMJyEG"},"outputs":[],"source":["class VQGanMonitor(keras.callbacks.Callback):\n","  def __init__(self, test_ds):\n","    self.test_ds = test_ds\n","\n","  def on_epoch_end(self, epoch, logs=None):\n","    autoencoded = self.model.enc_dec(self.test_ds)\n","    autoencoded.numpy()\n","    for i in range(len(autoencoded)):\n","      img = keras.utils.array_to_img(autoencoded[i])\n","      img.save(os.path.join(img_output_path, f\"autoencoded_{epoch:03d}_{i}.png\"))\n","\n","class VQGanCheckpoint(keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    self.model.encoder.save_weights(model_path(\"encoder\"))\n","    self.model.decoder.save_weights(model_path(\"decoder\"))\n","    self.model.discriminator.save_weights(model_path(\"discriminator\"))\n","    eg_weights = self.model.eg_optimizer.get_weights()\n","    np.savez(optimizer_path(\"decoder\"), *eg_weights)\n","\n","    d_weights = self.model.d_optimizer.get_weights()\n","    np.savez(optimizer_path(\"discriminator\"), *d_weights)"]},{"cell_type":"markdown","metadata":{"id":"jZEbxht5JyEH"},"source":["Discriminator model"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669645150961,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"hjZEW7mMJyEH"},"outputs":[],"source":["def get_discriminator():\n","  # if os.path.exists(model_path(\"discriminator\")):\n","  #   return keras.models.load_model(model_path(\"discriminator\"), compile=False)\n","\n","  discriminator = keras.Sequential([\n","    keras.Input(shape=(*image_size, 3)),\n","    layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n","    layers.LeakyReLU(alpha=0.2),\n","    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n","    layers.LeakyReLU(alpha=0.2),\n","    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n","    layers.LeakyReLU(alpha=0.2),\n","    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n","    layers.LeakyReLU(alpha=0.2),\n","    layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n","    layers.LeakyReLU(alpha=0.2),\n","    layers.Flatten(),\n","    layers.Dropout(0.2),\n","    layers.Dense(2, activation=\"softmax\")\n","  ], name=\"discriminator\")\n","  \n","  if os.path.exists(model_path(\"discriminator\")):\n","    discriminator.load_weights(model_path(\"discriminator\"))\n","  \n","  return discriminator"]},{"cell_type":"markdown","metadata":{"id":"gzNcwL1lJyEI"},"source":["Conv building parts"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669645150961,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"Z_rHic0ZJyEI"},"outputs":[],"source":["def conv_block(filters, x):\n","  x = layers.Conv2D(filters, kernel_size=3, padding=\"same\", strides=2)(x)\n","  x = GroupNormalization()(x)\n","  x = layers.ReLU()(x)\n","  x = layers.Dropout(0.2)(x)\n","  x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n","  x = GroupNormalization()(x)\n","  x = layers.ReLU()(x)\n","  return x\n","\n","def residual_block(filters, x):\n","  residual = x\n","\n","  x = GroupNormalization()(x)\n","  x = Swish()(x)\n","  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n","\n","  x = GroupNormalization()(x)\n","  x = Swish()(x)\n","  x = layers.Dropout(0.2)(x)\n","  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n","\n","  return layers.Add()([residual, x])\n","\n","def downsample_block(filters, x):\n","  return layers.Conv2D(filters, kernel_size=3, strides=2, padding=\"same\")(x)\n","\n","def upsample_block(filters, x):\n","  (_, w, h, _) = x.shape\n","  x = layers.Resizing(h * 2, w * 2)(x)\n","  x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\")(x)\n","  return x\n","\n","def positional_encoding2d():\n","  def inner(inputs):\n","    _, w, h, c = inputs.shape\n","    batch_size = tf.shape(inputs)[0]\n","    x = tf.range(start=0, limit=w, delta=1)\n","    x = x / w\n","    x = tf.expand_dims(x, axis=0)\n","    assert x.shape == (1, w)\n","    x = tf.tile(x, multiples=[h, 1])\n","    assert x.shape == (w, h)\n","    x = tf.reshape(x, shape=(w, h, 1))\n","\n","    y = tf.range(start=0, limit=h, delta=1)\n","    y = y / h\n","    y = tf.expand_dims(y, axis=1)\n","    assert y.shape == (h, 1)\n","    y = tf.tile(y, [1, w])\n","    assert y.shape == (w, h)\n","    y = tf.reshape(y, shape=(w, h, 1))\n","\n","    indexes = tf.concat([x, y], axis=-1)\n","    assert indexes.shape == (w, h, 2)\n","\n","    indexes = tf.expand_dims(indexes, axis=0)\n","    indexes = tf.tile(indexes, [batch_size, 1, 1, 1])\n","\n","    return layers.Conv2D(c, kernel_size=1, strides=1, padding=\"same\")(indexes)\n","    # Todo: the sinusoidal way from All you need is attention\n","  return inner\n","\n","def self_attention(x, num_heads=1, key_dim=64):\n","  pos = positional_encoding2d()(x)\n","  x = layers.add([x, pos])\n","  x = layers.MultiHeadAttention(num_heads, key_dim, attention_axes=(2, 3))(x, x, x)\n","  return x"]},{"cell_type":"markdown","metadata":{"id":"zKgkFITOJyEI"},"source":["Encoder"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1669645150962,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"FtdwUnPFJyEJ"},"outputs":[],"source":["def get_encoder():\n","  # if os.path.exists(model_path(\"encoder\")):\n","  #   return keras.models.load_model(model_path(\"encoder\"), compile=False, custom_objects={\n","  #     \"VectorQuantization\": VectorQuantization,\n","  #     \"Swish\": Swish\n","  #   })\n","\n","  encoder_inputs = keras.Input(shape=(*image_size, 3))\n","  x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(encoder_inputs)\n","  \n","  x = residual_block(filters, x)\n","  x = downsample_block(filters * 2, x)\n","  x = residual_block(filters * 2, x)\n","  x = downsample_block(filters * 4, x)\n","\n","  x = residual_block(filters * 4, x)\n","  x = self_attention(x, num_heads=3, key_dim=filters * 4)\n","  x = residual_block(filters * 4, x)\n","\n","  x = GroupNormalization()(x)\n","  x = Swish()(x)\n","  x = layers.Conv2D(embedding_dim, kernel_size=3, padding=\"same\")(x)\n","  x = VectorQuantization(embedding_length, embedding_dim)(x)\n","  encoder = keras.models.Model(encoder_inputs, x, name=\"encoder\")\n","\n","  if os.path.exists(model_path(\"encoder\")):\n","    encoder.load_weights(model_path(\"encoder\"))\n","\n","  return encoder"]},{"cell_type":"markdown","metadata":{"id":"nyhViOWdJyEJ"},"source":["Decoder"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669645150962,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"gDqHWaqcJyEJ"},"outputs":[],"source":["def get_decoder():\n","  # if os.path.exists(model_path(\"decoder\")):\n","  #   return keras.models.load_model(model_path(\"decoder\"), compile=False, custom_objects={\n","  #     \"Swish\": Swish\n","  #   })\n","\n","  decoder_input = keras.Input(shape=(image_size[0]//4, image_size[1]//4, embedding_dim))\n","  x = layers.Conv2D(filters * 4, 3, padding=\"same\")(decoder_input)\n","\n","  x = residual_block(filters * 4, x)\n","  x = self_attention(x, num_heads=3, key_dim=filters * 4)\n","  x = residual_block(filters * 4, x)\n","\n","  x = residual_block(filters * 4, x)\n","  x = upsample_block(filters * 2, x)\n","  x = residual_block(filters * 2, x)\n","  x = upsample_block(filters, x)\n","\n","  x = GroupNormalization()(x)\n","  x = Swish()(x)\n","  decoder_output = layers.Conv2D(3, 3, padding=\"same\")(x)\n","\n","  decoder = keras.models.Model(decoder_input, decoder_output, name=\"decoder\")\n","  \n","  if os.path.exists(model_path(\"decoder\")):\n","    decoder.load_weights(model_path(\"decoder\"))\n","\n","  return decoder"]},{"cell_type":"markdown","metadata":{"id":"mbp5m6qaJyEK"},"source":["Build model"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5622,"status":"ok","timestamp":1669645156578,"user":{"displayName":"Daniel Lindén","userId":"05917465914189297115"},"user_tz":-60},"id":"wJ-gUf9uJyEK","outputId":"8fa8ba3f-dd98-4270-9fe0-226b2f5e9a20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"encoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 224, 224, 32  896         ['input_2[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization (GroupNorm  (None, 224, 224, 32  64         ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," swish (Swish)                  (None, 224, 224, 32  0           ['group_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 224, 224, 32  9248        ['swish[0][0]']                  \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization_1 (GroupNo  (None, 224, 224, 32  64         ['conv2d_1[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," swish_1 (Swish)                (None, 224, 224, 32  0           ['group_normalization_1[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," dropout (Dropout)              (None, 224, 224, 32  0           ['swish_1[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 224, 224, 32  9248        ['dropout[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," add (Add)                      (None, 224, 224, 32  0           ['conv2d[0][0]',                 \n","                                )                                 'conv2d_2[0][0]']               \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 112, 112, 64  18496       ['add[0][0]']                    \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization_2 (GroupNo  (None, 112, 112, 64  128        ['conv2d_3[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," swish_2 (Swish)                (None, 112, 112, 64  0           ['group_normalization_2[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 112, 112, 64  36928       ['swish_2[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization_3 (GroupNo  (None, 112, 112, 64  128        ['conv2d_4[0][0]']               \n"," rmalization)                   )                                                                 \n","                                                                                                  \n"," swish_3 (Swish)                (None, 112, 112, 64  0           ['group_normalization_3[0][0]']  \n","                                )                                                                 \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 112, 112, 64  0           ['swish_3[0][0]']                \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 112, 112, 64  36928       ['dropout_1[0][0]']              \n","                                )                                                                 \n","                                                                                                  \n"," add_1 (Add)                    (None, 112, 112, 64  0           ['conv2d_3[0][0]',               \n","                                )                                 'conv2d_5[0][0]']               \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 56, 56, 128)  73856       ['add_1[0][0]']                  \n","                                                                                                  \n"," group_normalization_4 (GroupNo  (None, 56, 56, 128)  256        ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," swish_4 (Swish)                (None, 56, 56, 128)  0           ['group_normalization_4[0][0]']  \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 56, 56, 128)  147584      ['swish_4[0][0]']                \n","                                                                                                  \n"," group_normalization_5 (GroupNo  (None, 56, 56, 128)  256        ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," swish_5 (Swish)                (None, 56, 56, 128)  0           ['group_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 56, 56, 128)  0           ['swish_5[0][0]']                \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 56, 56, 128)  147584      ['dropout_2[0][0]']              \n","                                                                                                  \n"," add_2 (Add)                    (None, 56, 56, 128)  0           ['conv2d_6[0][0]',               \n","                                                                  'conv2d_8[0][0]']               \n","                                                                                                  \n"," tf.compat.v1.shape (TFOpLambda  (4,)                0           ['add_2[0][0]']                  \n"," )                                                                                                \n","                                                                                                  \n"," tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n"," ingOpLambda)                                                                                     \n","                                                                                                  \n"," tf.tile (TFOpLambda)           (None, 56, 56, 2)    0           ['tf.__operators__.getitem[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 56, 56, 128)  384         ['tf.tile[0][0]']                \n","                                                                                                  \n"," add_3 (Add)                    (None, 56, 56, 128)  0           ['add_2[0][0]',                  \n","                                                                  'conv2d_9[0][0]']               \n","                                                                                                  \n"," multi_head_attention (MultiHea  (None, 56, 56, 128)  197888     ['add_3[0][0]',                  \n"," dAttention)                                                      'add_3[0][0]',                  \n","                                                                  'add_3[0][0]']                  \n","                                                                                                  \n"," group_normalization_6 (GroupNo  (None, 56, 56, 128)  256        ['multi_head_attention[0][0]']   \n"," rmalization)                                                                                     \n","                                                                                                  \n"," swish_6 (Swish)                (None, 56, 56, 128)  0           ['group_normalization_6[0][0]']  \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 56, 56, 128)  147584      ['swish_6[0][0]']                \n","                                                                                                  \n"," group_normalization_7 (GroupNo  (None, 56, 56, 128)  256        ['conv2d_10[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," swish_7 (Swish)                (None, 56, 56, 128)  0           ['group_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 56, 56, 128)  0           ['swish_7[0][0]']                \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 56, 56, 128)  147584      ['dropout_3[0][0]']              \n","                                                                                                  \n"," add_4 (Add)                    (None, 56, 56, 128)  0           ['multi_head_attention[0][0]',   \n","                                                                  'conv2d_11[0][0]']              \n","                                                                                                  \n"," group_normalization_8 (GroupNo  (None, 56, 56, 128)  256        ['add_4[0][0]']                  \n"," rmalization)                                                                                     \n","                                                                                                  \n"," swish_8 (Swish)                (None, 56, 56, 128)  0           ['group_normalization_8[0][0]']  \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 56, 56, 4)    4612        ['swish_8[0][0]']                \n","                                                                                                  \n"," vector_quantization (VectorQua  (None, 56, 56, 4)   8192        ['conv2d_12[0][0]']              \n"," ntization)                                                                                       \n","                                                                                                  \n","==================================================================================================\n","Total params: 988,676\n","Trainable params: 988,676\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"decoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 56, 56, 4)]  0           []                               \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 56, 56, 128)  4736        ['input_3[0][0]']                \n","                                                                                                  \n"," group_normalization_9 (GroupNo  (None, 56, 56, 128)  256        ['conv2d_13[0][0]']              \n"," rmalization)                                                                                     \n","                                                                                                  \n"," swish_9 (Swish)                (None, 56, 56, 128)  0           ['group_normalization_9[0][0]']  \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 56, 56, 128)  147584      ['swish_9[0][0]']                \n","                                                                                                  \n"," group_normalization_10 (GroupN  (None, 56, 56, 128)  256        ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," swish_10 (Swish)               (None, 56, 56, 128)  0           ['group_normalization_10[0][0]'] \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 56, 56, 128)  0           ['swish_10[0][0]']               \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 56, 56, 128)  147584      ['dropout_4[0][0]']              \n","                                                                                                  \n"," add_5 (Add)                    (None, 56, 56, 128)  0           ['conv2d_13[0][0]',              \n","                                                                  'conv2d_15[0][0]']              \n","                                                                                                  \n"," tf.compat.v1.shape_1 (TFOpLamb  (4,)                0           ['add_5[0][0]']                  \n"," da)                                                                                              \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," tf.tile_1 (TFOpLambda)         (None, 56, 56, 2)    0           ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 56, 56, 128)  384         ['tf.tile_1[0][0]']              \n","                                                                                                  \n"," add_6 (Add)                    (None, 56, 56, 128)  0           ['add_5[0][0]',                  \n","                                                                  'conv2d_16[0][0]']              \n","                                                                                                  \n"," multi_head_attention_1 (MultiH  (None, 56, 56, 128)  197888     ['add_6[0][0]',                  \n"," eadAttention)                                                    'add_6[0][0]',                  \n","                                                                  'add_6[0][0]']                  \n","                                                                                                  \n"," group_normalization_11 (GroupN  (None, 56, 56, 128)  256        ['multi_head_attention_1[0][0]'] \n"," ormalization)                                                                                    \n","                                                                                                  \n"," swish_11 (Swish)               (None, 56, 56, 128)  0           ['group_normalization_11[0][0]'] \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 56, 56, 128)  147584      ['swish_11[0][0]']               \n","                                                                                                  \n"," group_normalization_12 (GroupN  (None, 56, 56, 128)  256        ['conv2d_17[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," swish_12 (Swish)               (None, 56, 56, 128)  0           ['group_normalization_12[0][0]'] \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 56, 56, 128)  0           ['swish_12[0][0]']               \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 56, 56, 128)  147584      ['dropout_5[0][0]']              \n","                                                                                                  \n"," add_7 (Add)                    (None, 56, 56, 128)  0           ['multi_head_attention_1[0][0]', \n","                                                                  'conv2d_18[0][0]']              \n","                                                                                                  \n"," group_normalization_13 (GroupN  (None, 56, 56, 128)  256        ['add_7[0][0]']                  \n"," ormalization)                                                                                    \n","                                                                                                  \n"," swish_13 (Swish)               (None, 56, 56, 128)  0           ['group_normalization_13[0][0]'] \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 56, 56, 128)  147584      ['swish_13[0][0]']               \n","                                                                                                  \n"," group_normalization_14 (GroupN  (None, 56, 56, 128)  256        ['conv2d_19[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," swish_14 (Swish)               (None, 56, 56, 128)  0           ['group_normalization_14[0][0]'] \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 56, 56, 128)  0           ['swish_14[0][0]']               \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 56, 56, 128)  147584      ['dropout_6[0][0]']              \n","                                                                                                  \n"," add_8 (Add)                    (None, 56, 56, 128)  0           ['add_7[0][0]',                  \n","                                                                  'conv2d_20[0][0]']              \n","                                                                                                  \n"," resizing (Resizing)            (None, 112, 112, 12  0           ['add_8[0][0]']                  \n","                                8)                                                                \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 112, 112, 64  73792       ['resizing[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization_15 (GroupN  (None, 112, 112, 64  128        ['conv2d_21[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," swish_15 (Swish)               (None, 112, 112, 64  0           ['group_normalization_15[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 112, 112, 64  36928       ['swish_15[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization_16 (GroupN  (None, 112, 112, 64  128        ['conv2d_22[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," swish_16 (Swish)               (None, 112, 112, 64  0           ['group_normalization_16[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 112, 112, 64  0           ['swish_16[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 112, 112, 64  36928       ['dropout_7[0][0]']              \n","                                )                                                                 \n","                                                                                                  \n"," add_9 (Add)                    (None, 112, 112, 64  0           ['conv2d_21[0][0]',              \n","                                )                                 'conv2d_23[0][0]']              \n","                                                                                                  \n"," resizing_1 (Resizing)          (None, 224, 224, 64  0           ['add_9[0][0]']                  \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_24 (Conv2D)             (None, 224, 224, 32  18464       ['resizing_1[0][0]']             \n","                                )                                                                 \n","                                                                                                  \n"," group_normalization_17 (GroupN  (None, 224, 224, 32  64         ['conv2d_24[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," swish_17 (Swish)               (None, 224, 224, 32  0           ['group_normalization_17[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_25 (Conv2D)             (None, 224, 224, 3)  867         ['swish_17[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,257,347\n","Trainable params: 1,257,347\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_26 (Conv2D)          (None, 112, 112, 64)      3136      \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 112, 112, 64)      0         \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 56, 56, 128)       131200    \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 56, 56, 128)       0         \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 28, 28, 128)       262272    \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 28, 28, 128)       0         \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 14, 14, 128)       262272    \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 128)       0         \n","                                                                 \n"," conv2d_30 (Conv2D)          (None, 7, 7, 128)         262272    \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 7, 7, 128)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dropout_8 (Dropout)         (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 2)                 12546     \n","                                                                 \n","=================================================================\n","Total params: 933,698\n","Trainable params: 933,698\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"data":{"text/plain":["<keras.optimizers.optimizer_v2.adam.Adam at 0x260d2300700>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["encoder = get_encoder()\n","encoder.summary()\n","decoder = get_decoder()\n","decoder.summary()\n","discriminator = get_discriminator()\n","discriminator.summary()\n","\n","enc_dec = keras.Model(encoder.inputs, decoder(encoder.outputs))\n","\n","model = VQGAN(encoder, decoder, discriminator, discriminator_weight)\n","\n","def set_optimizer_weights(optimizer, model, name, **kwargs):\n","  optimizer = keras.optimizers.Adam(**kwargs)\n","\n","  if os.path.exists(optimizer_path(name)):\n","    file = np.load(optimizer_path(name))\n","    weights = [file[n] for n in file.files]\n","    optimizer._create_all_weights(model.trainable_variables)\n","    optimizer.set_weights(weights)\n","\n","  return optimizer\n","\n","model.compile(\n","  eg_optimizer=keras.optimizers.Adam(learning_rate=eg_learning_rate),\n","  d_optimizer=keras.optimizers.Adam(learning_rate=d_learning_rate),\n","  loss_fn=keras.losses.SparseCategoricalCrossentropy(),\n","  perceptual_loss_fn=perceptual_loss_fn,\n","  run_eagerly=runeager)\n","\n","set_optimizer_weights(model.eg_optimizer, enc_dec, \"decoder\")\n","set_optimizer_weights(model.d_optimizer, model.discriminator, \"discriminator\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}],"source":["enc_dec.save(f\"models/vqgan_faces_v{version}.keras\", include_optimizer=False)"]},{"cell_type":"markdown","metadata":{"id":"ZNl0f5K4JyEK"},"source":["Fit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4LP_GloJyEK","outputId":"0b18f0d6-68b1-4790-857d-31dc4e402c04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n","3164/3164 [==============================] - 1868s 574ms/step - d_loss: 0.0511 - eg_loss: 0.1254 - embedding_loss: 0.0152 - decode_disc_loss: 6.6190 - perceptual_loss: 0.0777 - adaptive_weight: 0.0050\n","Epoch 2/1000\n","3164/3164 [==============================] - 1836s 574ms/step - d_loss: 0.0506 - eg_loss: 0.1205 - embedding_loss: 0.0142 - decode_disc_loss: 6.6060 - perceptual_loss: 0.0760 - adaptive_weight: 0.0047\n","Epoch 3/1000\n","3164/3164 [==============================] - 1836s 574ms/step - d_loss: 0.0443 - eg_loss: 0.1208 - embedding_loss: 0.0137 - decode_disc_loss: 6.9099 - perceptual_loss: 0.0763 - adaptive_weight: 0.0046\n","Epoch 4/1000\n","3164/3164 [==============================] - 1836s 574ms/step - d_loss: 0.0446 - eg_loss: 0.1234 - embedding_loss: 0.0136 - decode_disc_loss: 7.0895 - perceptual_loss: 0.0761 - adaptive_weight: 0.0049\n","Epoch 5/1000\n","3164/3164 [==============================] - 1837s 574ms/step - d_loss: 0.0395 - eg_loss: 0.1248 - embedding_loss: 0.0139 - decode_disc_loss: 7.9252 - perceptual_loss: 0.0786 - adaptive_weight: 0.0044\n","Epoch 6/1000\n","3164/3164 [==============================] - 1836s 574ms/step - d_loss: 0.0461 - eg_loss: 0.1188 - embedding_loss: 0.0135 - decode_disc_loss: 7.1149 - perceptual_loss: 0.0744 - adaptive_weight: 0.0045\n","Epoch 7/1000\n","3164/3164 [==============================] - 1837s 574ms/step - d_loss: 0.0305 - eg_loss: 0.1277 - embedding_loss: 0.0134 - decode_disc_loss: 8.0676 - perceptual_loss: 0.0786 - adaptive_weight: 0.0046\n","Epoch 8/1000\n","3164/3164 [==============================] - 1836s 574ms/step - d_loss: 0.0356 - eg_loss: 0.1208 - embedding_loss: 0.0133 - decode_disc_loss: 7.8820 - perceptual_loss: 0.0756 - adaptive_weight: 0.0042\n","Epoch 9/1000\n","  67/3164 [..............................] - ETA: 29:35 - d_loss: 0.0232 - eg_loss: 0.1140 - embedding_loss: 0.0126 - decode_disc_loss: 7.2156 - perceptual_loss: 0.0733 - adaptive_weight: 0.0040"]}],"source":["# log_dir = \"logs/dbg\"\n","# tf.debugging.experimental.enable_dump_debug_info(\n","#     log_dir,\n","#     tensor_debug_mode=\"NO_TENSOR\",\n","#     circular_buffer_size=-1)\n","\n","callbacks = [\n","  VQGanMonitor(test_imgs),\n","  VQGanCheckpoint()\n","  # callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","]\n","model.fit(dataset, epochs=epochs, callbacks=callbacks)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"32f2fe102a4f10662d8c13f75131e1ba377b7194060421a642fdea27c55fc65a"}}},"nbformat":4,"nbformat_minor":0}
